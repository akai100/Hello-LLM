 ## 1. 什么是推理？

 + **推理 = 在已训练好的模型上，给定输入 token，持续预测下一个 token 的过程**

⚠️ 注意这个“持续”非常关键。

和训练最大的不同：

+ 推理：一次性算完整个序列

+ 推理：**个 token 一个 token 地算（autoregressive）**


## 2. Transformer 推理的真实执行流程

以 Decoder-only LLM（如 LLaMA） 为例：

推理分两大阶段

**✅ Prefill 阶段**

+ 输入：prompt（一次性多个 token）
+ 行为：
  + 全量 self-attention
  + 计算并 缓存 KV
+ 特点：
  + 计算量大
  + 并行度高
  + GPU 利用率高

**✅ Decode 阶段**

+ 输入：1 个新 token
+ 行为：
  + 用历史 KV Cache
  + 只算 query
+特点：
  + 计算量小
  + 串行
  + GPU 利用率低

📌 面试金句：

“推理性能瓶颈通常不在 prefill，而在 decode 阶段。”

## 3. Attention 在推理中到底算了什么？

以第 / 层 attention 原理：

+ Q: 当前 token

+ K/V：历史所有 token （来自 KV Cache）

计算：

```
Attention(Q, K, V) = softmax(QKᵀ / √d) · V
```

**推理时的关键变化**

+ **K/V 不再重复计算**

+ 每一层，每个 token，都要存 K/V

👉 这就是 KV Cache 的由来。

## 4. KV Cache 是什么？（面试必问）

定义：

+ KV Cache = 存储每一层、每一个历史 token 的 Key 和 Value

**为什么必须要？**

+ 否则 decode 阶段需要反复算历史 attention

+ 时间复杂度会从 O(n) 退化成 O(n²)

**为什么吃显存？**

假设：

+ Layers = 32

+ Hidden size = 4096

+ Tokens = 2048

👉 KV Cache ≈ **几十 GB 级别**

📌 面试表达：

“KV Cache 本质上是用显存换时间，是推理阶段最大的显存消耗来源。”

## 5. 推理为什么这么慢？（系统视角）

我们**用性能工程的语言**说：

**Decode 阶段的三大瓶颈**

1️⃣ 串行依赖

+ 下一个 token 必须等上一个 token

2️⃣ Memory Bound

+ Attention 主要受限于 HBM 带宽

+ 不是算力不够，是数据搬得慢

3️⃣ Batch 利用率低

+ 不同请求长度不同

+ GPU 容易“空转”

📌 这三点，是后面 vLLM 所有优化的出发点。

## 6. 一个「大厂级」总结模板（你要背）

“LLM 推理是一个自回归过程，通常分为 prefill 和 decode 两个阶段。

Prefill 阶段并行度高、计算密集，而 decode 阶段是逐 token 的串行过程，性能瓶颈明显。

为了避免重复计算，推理会缓存每一层的 KV Cache，但这也导致显存占用成为主要限制。

因此，大模型推理优化的核心在于 提升 **decode 阶段的 GPU 利用率并降低 KV Cache 的显存压力。”**

## 7. 本模块作业（非常重要）

你现在需要**动笔思考**，这决定你后面学得快不快。

**作业 1（必做）**

**👉 用你自己的话回答：**

为什么 LLM 推理阶段，decode 比 prefill 更难优化？

（不用太长，5–8 行即可）

+ Prefill 阶段一次性处理多个 token，计算密集，GPU 并行度高，因此比较容易把 GPU 跑满。
+ 而 decode 阶段每次只生成一个 token，计算量小，并且存在严格的前后依赖，无法并行展开。
+ 同时 decode 阶段需要频繁访问历史 KV Cache，整体更偏向 memory bound，
+ 再加上不同请求生成长度不同，batch 难以保持一致，
+ 所以 decode 阶段比 prefill 更难优化。

**作业 2（加分）**

👉 粗略估算一个 7B 模型：

32 层

hidden size = 4096

seq len = 2048

KV Cache 大约占多少显存？
（允许你估算，不要求精确）
