## 相关面试题

### 在LLM的推理阶段，有哪些常见的解码策略？请解释 Greedy Search, Beam Search, Top-K Sampling 和 Nucleus Sampling (Top-P) 的原理和优缺点。

在LLM的推理（或称解码）阶段，模型会生成一个词元概率分布，解码策略决定了如何从这个分布中选择下一个词元。常见的策略可以分为确定性和随机性两类。

**1. Greedy Search (贪心搜索)**

+ 原理： 在每个时间步，总是选择当前概率分布中概率最高的那个词元作为输出。
+ 优点：
  + 速度快： 计算开销最小，实现最简单。
+ 缺点：
  + 局部最优： 每一步的“贪心”选择可能导致整个序列不是全局最优的。一个高概率的词后面可能跟着一系列低概率的词，最终序列的总概率反而不高。
  + 缺乏多样性： 输出是完全确定的，对于同一个输入，每次生成的结果都一样，内容往往比较呆板、重复。
  
**2. Beam Search (集束搜索)**
+ 原理： 这是对贪心搜索的改进。它在每个时间步会保留 k 个（ k 称为 "beam width" 或 "beam size"）最有可能的候选序列。在下一步，它会从这 k 个候选序列出发，生成所有可能的下一个词元，然后从所有这些扩展出的新序列中，再次选出累计概率最高的 k 个。最后，从最终的 k
 个完整序列中选择最优的一个。
+ 优点：
  + 质量更高： 通过探索更广的搜索空间，通常能找到比贪心搜索概率更高、质量更好的序列。
+ 缺点：
  + 计算成本高： 需要维护 k个候选序列，计算和内存开销是贪心搜索的 k 倍。
  + 仍然倾向于安全和高频： 优化目标是全局概率，这使得它还是倾向于生成常见、安全的句子，可能缺乏创造性，并且在长文本生成中容易出现重复。

**3. Top-K Sampling (Top-K 采样)**

+ 原理： 这是一种随机采样策略。在每个时间步，不再是选择最优的，而是：
  a. 从整个词汇表的概率分布中，筛选出概率最高的 K 个词元。
  b. 将这 K 个词元的概率进行归一化（使它们的和为1）。
  c. 在这 K 个词元中，根据新的概率分布进行随机采样。
+ 优点：
  + 增加多样性： 引入了随机性，使得生成内容更加丰富、有趣和不可预测。
  + 避免低概率词： 通过限制在Top-K范围内，过滤掉了那些概率极低、可能不通顺或奇怪的词元。
+ 缺点：
  + K值固定： K 是一个固定的超参数。当概率分布很尖锐时（模型非常确定下一个词），一个大的K可能会引入不相关的词；当概率分布很平坦时（模型不确定），一个小的K可能会限制模型的选择。

**4. Nucleus Sampling / Top-P Sampling (核心采样)**

+ 原理： 这是对Top-K采样的改进，它使用一个动态的候选词元集。
  a. 将所有词元按概率从高到低排序。
  b. 从概率最高的词元开始，逐个累加它们的概率，直到总概率之和超过一个预设的阈值 p（例如 p = 0.95）。
  c. 这个累加过程中包含的所有词元构成了“核心（Nucleus）”候选集。
  d. 然后，在这个动态大小的候选集中，根据它们的原始概率进行归一化和随机采样。
+ 优点：
  + 自适应候选集： 候选集的大小会根据上下文动态变化。当模型对下一个词非常确定时，概率分布尖锐，可能只有一两个词的概率和就超过了 p，候选集就很小，生成更精确；当模型不确定时，概率分布平坦，需要包含更多词才能达到 p，候选集就变大，允许更多探索。
  + 兼顾质量与多样性： 相比Top-K，它是一种更原则性和鲁棒性的方法，是目前大多数LLM应用默认的采样策略。
