## 1. 什么是剪枝？为什么要剪枝？

在尽量不明显降低模型性能的前提下，删除模型中不重要的参数、结构或计算路径，从而降低：

+ 参数量

+ 推理延迟

+ 显存/内存占用

+ 推理能耗

**对大模型（LLM）尤其重要

以 Transformer 为例：

+ 参数规模：10B / 70B / 100B+

+ 推理瓶颈：显存 & 带宽

+ 剪枝 != 量化

  + 剪枝：减少结构或参数数量
  
  + 量化：降低数值精度（FP16 → INT8

👉 实际部署中：剪枝 + 量化 + KV Cache 优化一起用

## 2. 剪枝的基本分类

### 2.1 非结构化剪枝

剪掉单个权重

+ 按权重大小（|w| 小 → 不重要）

+ 产生大量 0

+ 参数减少，但不利于硬件加速

✅ 优点：

+ 简单

+ 理论效果好

❌ 缺点：

+ 稀疏矩阵在 GPU 上不友好

+ 实际加速有限

### 2.2 结构化剪枝

**剪掉完整结构单元**

+ Attention Head

+ FFN 的 hidden neuron

+ 整个 Channel / Layer

📌 示例：

+ 12 个 head → 剪到 8 个

+ FFN：4096 → 3072
✅ 优点：

+ 真正减少计算量

+ 硬件友好

❌ 缺点：

+ 设计复杂

+ 更容易掉点

### 2.3 半结构化剪枝（Semi-structured）

介于两者之间，例如：

+ N:M 剪枝（2:4, 4:8）

+ NVIDIA Ampere 原生支持

## 3. 剪枝的判断

### 3.1 基于权重大小

最经典方法：

 $importance(w)=|w|$

+ 小权重 -> 不重要

+ L1/L2

📌 非结构化剪枝常用

### 3.2 基于梯度 / Hessian（二阶信息）

考虑 loss 的影响：

 $\delta L \approx \frac{1}{2}h_{ii}w_{i}^{2}$

代表方法：

+ Optimal Brain Damage（OBD）

+ Optimal Brain Surgeon（OBS）

❌ 计算成本极高（大模型几乎不用）

### 3.3 基于激活值

常用于结构化剪枝：

+ head 输出均值

+ neuron activation 方差

+ channel 的 L1 norm

📌 LLM head 剪枝常用

### 3.4 基于任务敏感度

+ 剪掉某结构 → 测验证集 loss 增量

+ 增量小 → 可剪

❌ 成本极高，但准确

## 4. Transformer / LLM 中常见剪枝对象

### 4.1 Attention Head 剪枝

每层多头注意力：

 $MultiHead(Q,K,V)=Concat(head_1​,...,head_h​)$

不是所有 head 都重要

判据：

+ head 输出范数

+ attention entropy

+ 对 loss 的影响

📌 常见结论：

很多 head 是冗余的，尤其是中间层

### 4.2 FFN 剪枝（最常见 & 最有效）

FFN 占 Transformer 60%+ 参数

 $FFN(x)=W2_2σ(W_1​x)$

剪：

+ hidden dimension

+ neuron（structured）

📌 例：

+ 4096 → 3072

+ 几乎线性减少计算量

### 4.3 Layer 剪枝（Layer Dropping）

+ 剪掉整个 Transformer block

+ 类似 Distillation

📌 常见做法：

+ 每隔几层剪一层

+ 训练时引入 stochastic depth

### 4.4 Embedding / Vocabulary 剪枝

+ 任务相关（如领域模型）

+ 很少用于通用 LLM

## 5. 剪枝流程（工业级）

### 5.1 标准 Pipeline

```
预训练模型
   ↓
重要性评估
   ↓
剪枝（一次 or 逐步）
   ↓
微调（Fine-tuning / Recovery）
   ↓
部署
```

### 5.2 一次性 vs 逐步剪枝

+ 一次性：简单但风险大

+ 逐步剪枝（Iterative）：每次剪 5%~10%，效果更稳

