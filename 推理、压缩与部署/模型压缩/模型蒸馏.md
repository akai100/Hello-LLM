
模型蒸馏（knowledge Distillation, KD）是一种将大模型的“知识”压缩到小模型中的技术，目的是在尽量保持性能的前提下，显著降低模型规模、
推理延迟和计算成本。

## 1. 直观理解

让小模型模仿大模型的“思考方式”，而不只是模仿正确答案。

+ Teacher（教师模型）：大、准、慢

+ Student（学生模型）：小、快、部署友好

+ 蒸馏的关键

  学习 teacher 输出的“软概率分布”

## 2. 为什么蒸馏有效

### 2.1 硬标签 VS 软标签

+ 硬编码

```
猫 → [1, 0, 0, 0]
```

+ 软标签

```
猫 → [0.70, 0.20, 0.05, 0.05]
```

软标签包含了：

+ 类别之间的相似性

+ teacher 的不确定性

+ 暗含的高阶知识

## 3. 核心数学原理

### 3.1 Softmax + 温度参数（Temperature）

 $p_i=\frac{exp(z_i/T)}{\sum{j}{}{exp(z_j/T)}}$

+ T = 1：普通 softmax

+ T > 1: 分布更平滑（蒸馏常用）

经验值： $T = 2 ~ 5$

### 3.2 蒸馏损失函数（标准形式）

 $L=\alpha\cdot L_{CE}(y,p_s)+(1-\alpha)\cdot T^2\cdot L_{KL}(p_{t}^{T},p_{s}^{T})$

+ $p_{t}^{T}$

  teacher 的软输出

+ $p_{s}^{T}$

  student 的软输出

+ $CE$

  交叉熵（硬标签）

+ $KL$

  KL 散度

+ $\alpha$

  权重平衡

为什么乘 $T^2$：保持梯度尺度稳定

### 3.3 数学意义

 $\alpha CE(y,p_s)$ : 监督学习；

 $T^2KL(p_{t}^{T}||p_{s}^{T}$：知识蒸馏

 其中：

 + $p_{t}^{T}$：teacher 在温度 $T$下的概率分布；

 + $p_{s}^{T}$：student 在温度 $T$下的概率分布；

**1. KL 散度的本质**

   $KL(p_t||p_s)=\sum_{i}p_t(i)log\frac{p_t(i)}{p_s(i)}$

数学意义：student 用自己的分布去编码 tearcher 的输出时，多浪费了多少消息（bit）

+ KL = 0

  两个分布完全一致

+ KL 越大

  student 越 “误解” teacher

📌 蒸馏 = 最小化 student 对 teacher 的信息损失

**2. 为什么不是反过来**

为什么不用 $KL(p_s||p_t)$

+ 会

  + 强惩罚 student 探索 teacher 低概率区域
 
  + 导致 mode collapse

+ 实际效果更差

Forward KL（teacher → student）

+ 保留 teacher 的多模态结构

+ 更“宽容”

#### 3.3.2 概率建模视觉：在拟合 “完整条件分布”

**1. 普通监督学习学的是什么？**

 $CE(y,p_s)=-logp_s(y|x)$

只关心：正确答案的概率够不够大？

本质问题

+ 忽略：

  + 其他类别之间的相对关系
 
  + 不确定性

**2. 蒸馏在学什么**

 $min KL(p_t(y|x)||p_s(y|x))$

 student 在学：teacher 对所有可能输出的完整条件分布

这是一个更强的目标：

| 目标  | 信息量       |
| --- | --------- |
| 硬标签 | log K bit |
| 软分布 | K-1 自由度   |

#### 3.3.3 优化视角：为什么梯度更好

**1. 梯度对比**

对 $logits z:$

硬标签 CE

 $\frac{∂L}{​∂z_i}​=p_s​(i)−1[i=y]$

+ 只有一个类别有强信号

+ 梯度稀疏

**蒸馏 KL**

 $\frac{∂L}{∂z_i}=\frac{1}{T}(p_{s}^{T}(i) - p_{t}^{T}(i))$

+ 所有类别都有梯度

+ 梯度更平滑

+ 更利于小模型优化

📌 这也是蒸馏对小模型特别友好的原因

**2. 温度 $T$的数学意义

**Softmax + 温度**

 $p_{i}^{T}=\frac{e^{z_i/T}}{\sum_{j}{}{e^{z_j/T}}}$

+ 𝑇 ↑ → 分布趋于均匀

+ 放大 logits 之间的相对差异信息

**为什么需要*乘* $T^2$



## 4. 完整训练流程

**Step 1：训练 Teacher**

+ 大模型

+ 性能优先

+ 可使用 ensemble / 数据增强

**Step 2：冻结 Teacher**

+ 推理模式

+ 不参与反向传播

**Step 3：训练 Student （蒸馏）**

```
输入 x
 ├─ Teacher → soft logits (T)
 └─ Student → logits
      ├─ CE(student, y)
      └─ KL(student_T, teacher_T)
```

## 5. 常见蒸馏方式分类

### 5.1 Logits 蒸馏（最经典）

+ 模仿输出概率

+ 简单、通用

适用：分类 / NLP / 多分类任务

### 5.2 特征蒸馏（Feature Distillation）

+ 对齐中间特征

+ 常用于 CNN / ViT

 $L_{feat}=||f_t - f_s||_{2}^{2}$

适用于：目标检测、分割

### 5.3 关系蒸馏（Relation KD）

+ 学样本之间的相似度结构

+ 不强制特征维度一致

代表：

+ RKD

+ CRD

### 5.4 自蒸馏（Self Distillation）

+ teacher = student（不同层 / 不同 epoch）

特点：

+ 无需大模型

+ 常用于 ViT、ResNet

### 5.5 在线蒸馏

+ teacher 与 student 同时训练

+ 如：Deep Mutual Learning

## 6. 蒸馏在不同领域的应用

### 6.1 NLP

+ BERT → TinyBERT / DistilBERT

+ 常蒸馏：

  + logits

  + attention map
  
  + hidden states

### 6.2 CV

+ ResNet-152 → ResNet-50

+ ViT → MobileViT

+ 常结合 feature distillation

### 6.3 大模型（LLM）

+ GPT-4 -> 7B / 3B

+ Sequence-level KD

+ Policy Distillation（RLHF 场景）

## 7. 实践经验 & 调参建议（非常重要）

### 7.1 超参数推荐

| 参数         | 建议      |
| ---------- | ------- |
| T          | 2–5     |
| α          | 0.1–0.5 |
| 蒸馏 loss 占比 | 30%–70% |

### 7.2 常见坑

❌ teacher 太弱 → 蒸馏无收益

❌ student 太小 → 容量不足

❌ 只用 KL，不用 CE → 容易偏移标签

❌ 数据分布不一致 → 蒸馏失败

## 8. 蒸馏 VS 其他压缩方法

| 方法   | 是否需 teacher | 优点  | 缺点    |
| ---- | ----------- | --- | ----- |
| 蒸馏   | ✅           | 性能好 | 训练复杂  |
| 剪枝   | ❌           | 快   | 易掉点   |
| 量化   | ❌           | 推理快 | 精度敏感  |
| LoRA | ❌           | 低成本 | 非极致压缩 |

最佳实践：

蒸馏 + 量化 + 剪枝 = 工业级方案
