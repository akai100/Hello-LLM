
量化是降低大模型显存占用、提升推理/训练速度的核心技术。

## 核心原理

### 本质：用低精度替代高精度

大模型默认采用 32 位浮点数（FP32）存储参数，量化的核心是将高精度数据（FP32/FP16/BF16）转换为低精度数据（INT8/INT4/INT2），同时尽可能保留模型的精度。

**核心公式（以INT8 量化为例）**

$$x_{int8} = round(\frac{x_{fp32} - \text{min}}{scale}) + \text{zero\\_point}$$

反量化（低精度 -> 高精度，推理时）

$$x_{fp32} = (x_{int8} - \text{zero\\_point}) \times scale + \text{min}$$

其中：

+ scale：缩放因子， $scale=\frac{max-min}{255}$(INT8 范围[-128, 127]);
+ zero_point：零点，将浮点数的 0 映射到整数范围的中点；
+ min/max：张量的最小值 / 最大值（量化校准用）;

## 主流量化分类

### 把 BIT 数

| 类型   | bit | 特点     |
| ---- | --- | ------ |
| FP16 | 16  | 精度高，慢  |
| INT8 | 8   | 工业常用   |
| INT4 | 4   | LLM 主流 |
| INT2 | 2   | 研究为主   |

### 按量化时机

#### PTQ（Post-Training Quantization）

训练后再量化（最常用）

+ 不需要再训练

+ 快速、工程友好

+ 精度略有损失

常见方法：

+ GPTQ

+ AWQ

+ SmoothQuant

#### QAT（Quantization-Aware Training）

训练时量化

+ 精度最好

+ 训练成本高

+ 不适合超大模型

### 按量化粒度

| 粒度          | 示例                  | 特点         |
| ----------- | ------------------- | ---------- |
| Per-Tensor  | 一个 scale            | 简单但误差大     |
| Per-Channel | 每个 channel 一个 scale | LLM 主流     |
| Per-Group   | 32/64 一组            | GPTQ / AWQ |

## 典型 LLM 量化算法

### GPTQ

**核心思想：** 利用 Hessian 信息，最小化量化误差对输出的影响

+ 逐层量化

+ 贪心 + 校正误差

+ 支持 INT4 / INT8
