## 1. LLM as a Judge 评估时需要注意什么？

LLM-as-a-Judge（以大模型作为裁判） 是目前评估生成式任务（Open-ended generation）的主流方案。

使用它时，你必须解决模型自带的**系统性偏见**和**评估一致性**问题。以下是评估时需要注意的核心要点：

**1. 警惕四种核心偏见 (Biases)**

LLM 并不是完美的裁判，它们存在明显的心理倾向，面试中常被追问：

+ **长度偏见**

  模型倾向于给更长、更详细的回复打高分，即使这些回复包含废话或复读。

+ **位置偏见**

  在进行成对比较（Pairwise Comparison）时，模型往往更倾向于选择出现在第一个（或第二个）位置的答案。

  + 对策

    交换两个候选答案的位置，运行两次评估，如果结果不一致则判定为无效或重新评估。

+ **自我中心偏见 (Self-enhancement Bias)**

  模型往往给自家模型家族生成的回复打高分（例如 GPT-4 倾向于认为 GPT-3.5 的回答更好）。

+ **同质化偏见**

  模型偏好符合自己训练数据风格的回复，而忽略了内容本身的正确性。

**2. 提升评估质量的技巧**

为了让 LLM 评得更准，你需要像调优模型一样调优你的“裁判提示词”：

+ 引入思维链 (CoT)：在 Prompt 中要求裁判“**先写理由，再打分**”。如果先打分再写理由，分值往往会受到模型首词预测的随机性影响；先分析后结论更符合逻辑一致性。

+ **细粒度评分量表 (Rubric)**：不要只说“请给 1-10 分”。必须定义清楚：

  + 1 分代表：完全错误或有害。

  + 5 分代表：逻辑通顺但事实有误。

  + 10 分代表：完美且有启发性。

+ **提供参考答案 (Reference-based)**：如果可能，给裁判提供一份“金牌答案（Gold Standard）”，让它对比候选输出与标准答案的差异，这能显著降低幻觉导致的误判。

**3. 工程化落实**

在生产环境中，你还需要注意以下落地细节：

| 关注点 | 建议方案 |
|--------|----------|
| 一致性检查 | 对同一个样本进行多次评估（Self-consistency），取众数或平均值|
| 模型选择 | 裁判模型的等级必须高于被评估模型。通常用 GPT-4o 或 Claude 3.5 Sonnet 评测其他小模型 |
| 输出格式化 | 要求裁判以 JSON 格式输出，方便自动化脚本解析分数和理由 |
| 人工对齐 | 随机抽取 5%-10% 的结果进行人工审计，计算 Cohen's Kappa 系数，确保机器评分与人类偏好一致 |
