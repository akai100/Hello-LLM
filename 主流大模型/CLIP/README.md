
## 1. CLIP 的核心思想：对比学习

传统的图像分类模型（如 ResNet）是判别式的，只能识别训练集里预设的那几千个类别。而 CLIP 采用的是对比学习（Contrastive Learning）。

+ 数据源： 也就是所谓的“弱监督”，从互联网上抓取了 4 亿组“图片-文本”对 $(Image, Text)$。
+ 训练目标： 让模型学习到一个联合向量空间。在这个空间里，配对的图文向量距离越近越好，不配对的越远越好。

## 2. 模型结构与训练过程

CLIP 由两个主要部分组成：

+ Image Encoder： 负责提取图像特征（常用 ResNet 或 ViT）。
+ Text Encoder： 负责提取文本语义（常用 Transformer）。

训练步骤：

1. 特征提取： 一个 Batch 中有 $N$ 张图片和 $N$ 条文本，分别通过两个编码器得到特征向量。
   
2. 计算相似度： 计算这 $N$ 张图片和 $N$ 条文本之间的两两相似度（通常用余弦相似度），生成一个 $N \times N$ 的矩阵。

3. 损失函数： 使用 InfoNCE Loss（对数交叉熵损失）。

   + 正样本： 矩阵对角线上的元素（真实的图文配对）。

   + 负样本： 矩阵非对角线上的元素。

   + 目标： 最大化对角线相似度，最小化非对角线相似度。

## 3. CLIP 的两大杀手锏

A. Zero-Shot Learning（零样本学习）

这是 CLIP 最惊艳的地方。它不需要在特定数据集上微调，就能完成分类任务。

+ 做法： 将类别标签填入模板（如 "A photo of a {label}"），生成文本向量。

+ 匹配： 将测试图片输入 Image Encoder，看它和哪个文本向量最接近，那个就是预测类别。

B. 强大的稳健性（Robustness）

因为 CLIP 是在海量互联网数据上学习的语义，而不是死记硬背像素特征，所以它对素描图、漫画图、对抗性样本的识别能力远强于传统的 ImageNet 模型。
