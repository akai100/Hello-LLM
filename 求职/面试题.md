## 1. Flashattention的原理（原理答出来了，但面试官说还做了一个softmax的优化，使得可以让分块的qkv计算结果聚合，这点没答出来）

## 2. 分布式训练的一些方法？deepspeed支持哪些分布式训练？zero1，zero2，zero3的区别？

## 3. LLM训练中有哪些学习率和batch设置的经验trick？


**1. 学习率与 Batch Size 的“协同律”**

模型训练的稳定性很大程度上取决于**学习率/Batch Size** 的比例。

+ 线性缩放规则 (Linear Scaling Rule)

  当 Batch Size 扩大 $N$ 倍时，为了保持梯度方差一致，学习率通常也应扩大 $N$ 倍。

+ 平方根缩放规则 (Square Root Scaling)

  在某些场景（如超大规模模型）下，学习率按 $\sqrt{N}$ 增加会比线性缩放更稳定。

+ 临界 Batch Size (Critical Batch Size)

  研究发现，对于特定的模型和任务，存在一个“临界点”。在此点之前，增加 BS 能加速收敛；超过此点，增加 BS 的收益会迅速衰减（边际效应）。

**2. 学习率策略：不仅仅是 Cosine Decay**

目前主流的 LLM 训练多采用 WSD (Warmup-Stable-Decay) 或改进的 Cosine Decay。

+ Warmup(预热）

  + 技巧：在训练最开始的 100~2000 个 step 内，将 LR 从 0 线性增加到最大值（Peak LR）
 
  + 作用：防止模型在随机初始化阶段因为梯度过大而崩盘，帮助模型平稳进入 Loss 峡谷

+ Peak LR（峰值学习率）的经验值

  + 7B 级别模型：通常在 $3 \times 10^{-4}$ 到 $6 \times 10^{-4}$ 之
 
  + 更大的模型：随着参数量增加，Peak LR 需要相应减小（例如 70B 模型常设为 $1 \times 10^{-4}$ 到 $2 \times 10^{-4}$）。

+ WSD 调度器（DeepSeek 常用）

  + Warmup: 快速上升
 
  + Stable: 保持高学习率长时间运行
 
  + Decay: 在最后 10% 或 20% 的数据量阶段，快速衰减到零。这被证明能比传统 Cosine 获取更好的最终 Loss

**3. Batch Size 设置的 Trick**

+ 渐进式 Batch Size (Batch Size Ramp-up)：

  + 技巧：不要一开始就用 4M 或 8M 的 Global Batch Size。先从小 BS 开始（例如从 128 开始），随着训练步数逐步增加到目标 BS
 
  + 作用：在训练早期（模型还不稳定时）增加更新频率，在后期（需要更精确梯度方向时）增加单步样本量，能有效减少训练总时长

+ Micro Batch Size 与梯度累加 (Gradient Accumulation)

  + 技巧：单卡能跑多大就设多大（通常是 2 的幂次方，如 2, 4, 8），以榨干 GPU 计算核心（利用 Tensor Core 的对齐特性）
 
  + 计算公式：$\text{Global Batch Size} = \text{Micro Batch Size} \times \text{DP Size} \times \text{Accumulation Steps}$

**4. 其它实战经验建议**

+ Weight Decay (权重衰减)：通常固定在 0.1 左右。不要让它随 LR 缩放，保持稳定即可

+ Gradient Clipping (梯度裁剪)：通常设为 1.0。如果训练不稳定，可以尝试降到 0.5。如果经常触发裁剪，说明学习率可能设高了

+ Adafactor 或 AdamW：AdamW 是标配。注意 $\beta_2$ 的设置，对于超大规模训练，$\beta_2$ 设置为 0.95 或 0.98 有时比默认的 0.999 更能防止早期 Loss 震荡

总结：如何开始调优

1. 先定模型规模和数据量：根据 Scaling Law 估算所需的总 Tokens 数。

2. 选定 Batch Size：7B 模型通常选择 4M (4百万个 tokens) 作为全量训练的 BS。

3. 确定 Peak LR：参考同规模模型的论文（如 Llama 3 报告的值），先跑一个小比例的训练观察 Loss 是否下降顺滑。

4. 动态调整：如果 Loss 出现 Spikes（尖峰），调大 Warmup 步数或减小 Peak LR


## 4. 微调模型需要多大显存？

## 5. 为什么SFT之后感觉LLM傻了?

## 6. 如何将prompt和微调结合？

“微调决定了模型的‘底色’和‘专业深度’，而 Prompt 决定了模型在具体任务中的‘表现力’和‘灵活性’。”

+ 微调：解决的是模型“知不知道”（领域知识）和**“像不像”**（风格、格式、语气）的问题

+ Prompt：解决的是模型“当下要做什么”（具体指令）和**“利用哪些新信息”**（上下文）的问题

三个结合点：

1. 模板化指令微调

做法：在微调数据集中，将 Prompt 的固定格式（如 ### Instruction, ### Input, ### Response）直接打入模型参数。

价值：降低推理成本。微调后的模型不再需要冗长的 Few-shot（示例），只需一个简短的关键词就能触发复杂的行为

2. 结构化约束的固化

做法：如果业务需要模型稳定输出 JSON。在微调时，Prompt 始终包含“请按 JSON 格式输出”的指令。

价值：解决 Prompt Engineering 无法 100% 保证格式稳定的痛点，通过微调强制模型形成“格式肌肉记忆”

3. 检索增强（RAG）与微调的互补

做法：模型通过微调学习如何更好地利用 Prompt 中注入的检索上下文。

价值：微调让模型学会“如果 Prompt 里的参考资料和你的内置知识冲突，以 Prompt 为准”，这能显著减少幻觉

面试回答：

“在实际工程中，我通常遵循 ‘Prompt 为先，微调断后’ 的原则。

首先通过 Prompt Engineering 验证可行性并打磨出最佳指令模板；当 Prompt 达到瓶颈（如上下文太长导致成本过高，或指令遵循度不够稳定）时，再将这些经过验证的 Prompt 沉淀到训练集中进行微调。

这种结合方式既能发挥大模型泛化能力强的优势，又能确保在垂直业务场景下的稳定性和低延迟。”

## 7. 怎么从特别多的prompt验证你的prompt最好?

回答逻辑架构：

1. 建立评测集

验证的前提是有统一的“标尺”。

+ 筛选代表性数据：从海量数据中抽取 50-100 条具有代表性的样本（覆盖长尾场景、易错场景、核心业务场景）。

+ 确定标准答案 (Ground Truth)：对于有固定答案的任务，准备标准输出；对于开放式任务，定义好评分维度（如：准确性、简洁度、安全性）。

第一步，通过 LLM-as-a-Judge 在小规模评测集上进行快速扫描，剔除掉那些明显无法遵循指令或格式错误的 Prompt；

第二步，针对表现优秀的 Top 3-5 个 Prompt，进行 ELO 竞技场测试。我会关注模型在不同温度（Temperature）下的鲁棒性，确保它在生产环境中不会因为随机性而崩溃；

第三步，我会分析 Token 效率。如果在效果持平的情况下，Prompt A 比 Prompt B 短 30%，我会优先选择 A 以节省长期推理成本。”

2. 自动化评测 (Automated Evaluation)

当 Prompt 数量极多时，必须引入自动化工具：

+ LLM-as-a-Judge：利用更强大的模型（如 GPT-4o 或 Gemini 1.5 Pro）作为裁判。给裁判一份评分量表（Rubric），让它给不同 Prompt 的输出打分。

+ 指标计算：

  + 确定性任务：使用 BERTScore, ROUGE 或 Exact Match。

  + 分类任务：计算 Accuracy, F1-score。

  + 代码任务：使用 Pass@k。

3. 竞技场模式 (ELO Rating / Head-to-Head)

借鉴 LMSYS Chatbot Arena 的机制：

  + 两两对决 (A/B Testing)：让不同的 Prompt 针对同一输入产生结果，盲测对比。

  + 计算 ELO 分数：通过多次胜负关系，给每个 Prompt 算出一个排名分数，选出表现最稳健的那一个。

4. 成本与性能平衡 (Pareto Optimality)

最好的 Prompt 不仅仅是效果好，还要考虑：

  + Token 消耗：Prompt 是不是太长了？

  + 推理延迟：是否因为复杂的 Few-shot 导致模型响应变慢？

  + 稳定性（Variance）：多次运行同一 Prompt，结果的波动是否在可控范围内。

## 8. 你认为好的prompt 的范式是什么?

在面试或实际工程中，谈到 Prompt 范式，你不能只提“写清楚点”，而应该提出一套结构化、可复用的框架。目前业界公认最成熟、最易于扩展的范式是 CO-STAR 框架或 LangChain 典型的结构化 Prompt。

### 1. 结构化范式：CO-STAR 框架

这是一个非常扎实的逻辑框架，能确保你没有遗漏任何关键信息。

+ C: Context (上下文)：提供任务背景。模型需要知道它在为谁服务，以及为什么要做这件事。

+ O: Objective (目标)：清晰界定你要求模型执行的具体任务。

+ S: Style (风格)：指定写作风格（如：专业、幽默、简练）。

+ T: Tone (语气)：设定情感基调（如：鼓励性的、严肃的、中立的）。

+ A: Audience (受众)：明确输出是给谁看的（如：5岁小孩、资深架构师）。

+ R: Response (响应格式)：规定输出结构（如：JSON、Markdown、表格）。

### 2. 工程化范式：结构化提示词

这种范式类似于**“伪代码”**，利用 LLM 对 Markdown 层级结构的敏感性，是目前公认在复杂任务中表现最稳健的写法。

```Markdown
# Role: [角色名称]
## Profile:
- Author: [作者]
- Version: 1.0
- Language: 中文
- Description: [简述角色功能]

## Goals:
1. [目标1]
2. [目标2]

## Constraints:
- [约束条件1: 禁用词等]
- [约束条件2: 格式要求]

## Workflows:
1. 第一步：分析输入内容...
2. 第二步：检索相关知识...
3. 第三步：生成最终回复...

## Initialization:
作为[角色], 严格遵守[Constraints], 执行[Workflows], 请输入您的问题。
```

### 3. 进阶技巧：赋予 Prompt “灵魂”

好的范式除了结构，还应包含以下三个核心技术：

+ **Few-shot（少样本提示）**：在 Prompt 中给 2-3 个“问题-答案”对。这是提升模型遵循度最有效的方法，没有之一。

+ **Chain of Thought (CoT，思维链)*(*：添加“Let's think step by step”或要求模型“先分析逻辑，再给出结论”。这能显著提升逻辑推理任务的准确率。

+ **Delimiter（分隔符）**：使用 ###、""" 或 --- 将指令、参考资料和用户输入隔开。这能防止模型产生指令混淆。

### 4. 面试高分回答逻辑

如果你在面试中回答，可以这样总结：

“我认为好的 Prompt 范式应该具备 **‘高内聚、低耦合’** 的特点。

首先，它应该是结构化的，利用 Markdown 标题区分角色、目标和约束，因为大模型对结构化标记的注意力权重更高。

其次，它应该是模块化的，方便将动态的上下文（如 RAG 检索到的内容）注入到特定的模块中。

最后，一个优秀的生产级 Prompt 必须包含负向约束（Negative Constraints）和输出格式控制，以确保下游系统可以稳定解析模型返回的结果。”

## 9. 数据清洗流程

### 1. 数据探索与质量评估 (Exploration & Assessment)

在动手清洗之前，先通过统计学方法理解数据的“病症”。

+ 缺失值分析：统计每列的缺失比例（NaN/Null）。
+ 异常值检测：利用 $3\sigma$ 原则或 IQR (四分位距) 检测离群点。
+ 一致性检查：检查是否存在重复记录或逻辑冲突（如：年龄为负数）。

### 2. 处理缺失值 (Missing Data)
 
缺失值是数据集中最常见的“洞”。

+ 删除： 如果缺失比例极高（如超过 50%）或该行数据关键信息缺失，直接剔除。

+ 填充 (Imputation)： * 数值型：使用均值、中位数或众数填充。

  + 分类参数：使用“Unknown”或众数填充。

+ 预测填充： 利用回归模型或 KNN 算法根据其他变量预测缺失值。

### 3. 处理重复数据 (De-duplication)

+ 完全重复： 整行数据一模一样，通常由系统录入错误引起。

+ 逻辑重复： 主键相同但内容略有差异，需根据时间戳保留最新的一条。

### 4. 异常值处理 (Outliers)

异常值不一定是错的，但它们会干扰分析结果。

+ 识别： 使用 $Z-score$（通常超过 $\pm3$）或 $IQR$（四分位距）判定。
+ 处理策略： * 若是录入错误（如年龄 200 岁），直接修正或删除。
  + 若是真实存在的极端值，可考虑进行盖帽处理（Winsorization）或单独分析

### 5. 数据标准化与统一 (Standardization)

让数据说话的“语言”保持一致。

+ 单位统一： 比如将“厘米”和“米”全部换算为“米”。

+ 格式统一： 日期格式（2023-01-01 vs 01/01/23），性别（M/F vs Male/Female）。

+ 文本清洗： 去除多余空格、统一大小写、纠正明显的错别字。

### 6. 数据验证 (Validation)这是最后一道防线。

+ 交叉验证： 比如“年龄”字段与“出生日期”字段是否匹配。
+ 业务逻辑检查： 比如“订单总额”是否等于“单价 $\times$ 数量”。

## 10. 介绍 clip

## 11. 如何解决幻觉、复读机问题

### 1. 幻觉问题 (Hallucination)

幻觉是指模型生成了看似合理但与事实不符、或逻辑自相矛盾的内容。

核心解决对策

+ RAG (检索增强生成)： 这是目前性价比最高的方法。给模型塞一本“说明书”（外部知识库），强制它根据检索到的相关片段进行回答。

    公式： 模型 + 外部私域知识 = 准确性提升。

+ Prompt Engineering (提示词工程)： 在提示词中明确要求：“如果你不知道，请回答不知道”、“必须基于提供的文档回答”。

+ COT (思维链) 引导：要求模型“一步步思考”。通过增加推理步骤，减少在复杂逻辑推导中产生的“幻觉偏差”。

+ 验证机制 (Self-Correction)： 引入第二个模型（或同一模型的不同实例）对生成结果进行事实检查（Fact-checking）。

### 2. “复读机”问题 (Repetition)

复读机问题通常表现为模型在短语、句子甚至段落层面的死循环。

核心解决对策

+ 采样策略调整 (Decoding Strategy)：
  + Temperature (温度)： 适当调高温度，增加生成内容的不确定性。
  + Top-P / Top-K： 限制采样范围，避免模型陷入概率极高的死循环单词中。
+ 惩罚机制 (Penalty Factors)：
  + Presence Penalty (存在惩罚)： 只要某个词出现过，就降低其再次出现的概率。
  + Frequency Penalty (频率惩罚)： 某个词出现的次数越多，受到的惩罚越重。
+ N-gram Blocking： 在推理阶段强制规定：如果连续 $N$ 个词已经出现过，则禁止下一次再出现这组词。
+ Logit Bias： 手动降低某些极易导致循环的关键词的 Logit 分数。

### 3. 面试加分点：系统性优化方案

在实际生产环境中，我们通常采用组合拳：

| 维度 | 解决方案 | 预期效果 |
|------|---------|-----------|
| 训练期 | 强化对齐（RLHF）、高质量指令微调（SFT） | 提升模型对“拒绝回答”边界的理解 |
| 推理期 | RAG + 采样参数优化 | 解决实时事实性与生成多样性 |
| 后处理 | 语义去重算法、内容过滤器 | 兜底拦截低质量的重复输出 |

总结金句解决幻觉靠 **“给它看书（RAG）”和“让它说实话（RLHF）”；解决复读机靠“加点随机性（Sampling）”和“设点禁区（Penalty）”**。

## 12. transformer内部的注意力机制的不同之处

### 1. 三种主要注意力机制的对比

| 机制类型 | 所在位置 | 查询项 (Q) | "键项/值项 (K, V)" | 主要目的 |
|---------|----------|-----------|-------------------|--------------|
| 自注意力 (Self-Attention) | Encoder / Decoder | 来自前一层输出 | 来自前一层输出 | 捕捉序列内部的语义联系和上下文。 |
| 掩码自注意力 (Masked Self-Attention) | Decoder | 来自前一层输出 | 来自前一层输出 |防止模型在生成当前词时“偷看”未来的信息。 |
| 交叉注意力 (Cross-Attention) | Decoder 中间层 | 来自 Decoder 上一层 | 来自 Encoder 的最终输出 | 让解码器建立起输出序列与输入序列之间的联系。 |

## 13. transformer前馈神经网络的结构，为什么会先提升维度后缩小维度？

### 1. FFN 的基本结构

在标准的 Transformer（如 BERT 或 GPT 系列）中，FFN 由两个线性变换层组成，中间夹着一个非线性激活函数（通常是 ReLU 或 GELU）。

其数学表达如下：

$$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

+ 第一层（升维）： 将输入的维度从 $d_{model}$（例如 512）扩展到 $d_{ff}$（通常是 2048，即 4 倍）。

+ 第二层（降维）： 将维度从 $d_{ff}$ 重新压缩回 $d_{model}$。

### 2. 为什么要“先升后降”？

这种“沙漏型”或“瓶颈型”的反向设计（中间厚、两头薄）主要出于以下几个深层原因：

A. 增强模型的非线性表达能力

如果只是单纯的线性变换，无论堆叠多少层，最终都等同于一层线性运算。非线性激活函数（如 ReLU）是模型学习复杂模式的关键。

+ 在更高维度的空间中，ReLU 能够捕捉到更多的“激活模式”。

+ 升维后再进行非线性变换，相当于在更高维的空间进行“切割”和特征组合，这能显著提升模型处理复杂逻辑的能力。

B. 模拟“记忆”存储（Key-Value 视角）

有一种主流的研究观点（如 Transformer Feed-Forward Layers Are Key-Value Memories）认为：

+ 第一层（升维层）充当 Key： 它负责识别输入序列中的特定模式。维数越高，能记住的模式就越多。

+ 第二层（降维层）充当 Value： 当第一层识别出某个模式后，第二层负责提取该模式对应的“语义信息”并将其整合回模型。

+ 比喻： 就像一个巨大的词典，升维是为了容纳更多的词条，降维是为了根据查到的结果输出精简的含义。

C. 特征解耦与稀疏性在低维空间中，特征往往高度纠缠。通过升维，可以将这些特征拉伸到高维空间，实现特征解耦。

+ 升维后，模型可以更容易地通过激活函数筛选出哪些特征是当前任务需要的（产生稀疏性）。

+ 降维则起到了一种“特征压缩”和“信息融合”的作用，只保留最有用的信号传递给下一层。

### 3. 为什么不一直保持高维度？

既然高维度效果好，为什么还要降回来？

1. 计算效率： 如果每一层都保持 $4 \times d_{model}$ 的维度，参数量和计算量会呈指数级增长，显存也吃不消。

2. 残差连接（Residual Connection）： Transformer 依靠残差连接来防止梯度消失。残差连接要求输入和输出的维度必须一致，因此降维是必须的操作。

面试核心总结

“升维是为了提供足够的空间进行复杂的特征提取和模式记忆；降维是为了压缩信息、降低计算开销，并满足残差连接的维度一致性。”

## 14. llama123的区别

## 15. 前沿LLM有了解哪些?国内LLM有了解哪些?你认为LLM推理能力的天花板现在是什么样的？

## 16. 大模型结构有哪些变化

## 17. 你认为训LLM最大的困难是什么

