## 1. Flashattention的原理（原理答出来了，但面试官说还做了一个softmax的优化，使得可以让分块的qkv计算结果聚合，这点没答出来）

## 2. 分布式训练的一些方法？deepspeed支持哪些分布式训练？zero1，zero2，zero3的区别？

## 3. LLM训练中有哪些学习率和batch设置的经验trick？


**1. 学习率与 Batch Size 的“协同律”**

模型训练的稳定性很大程度上取决于**学习率/Batch Size** 的比例。

+ 线性缩放规则 (Linear Scaling Rule)

  当 Batch Size 扩大 $N$ 倍时，为了保持梯度方差一致，学习率通常也应扩大 $N$ 倍。

+ 平方根缩放规则 (Square Root Scaling)

  在某些场景（如超大规模模型）下，学习率按 $\sqrt{N}$ 增加会比线性缩放更稳定。

+ 临界 Batch Size (Critical Batch Size)

  研究发现，对于特定的模型和任务，存在一个“临界点”。在此点之前，增加 BS 能加速收敛；超过此点，增加 BS 的收益会迅速衰减（边际效应）。

**2. 学习率策略：不仅仅是 Cosine Decay**

目前主流的 LLM 训练多采用 WSD (Warmup-Stable-Decay) 或改进的 Cosine Decay。

+ Warmup(预热）

  + 技巧：在训练最开始的 100~2000 个 step 内，将 LR 从 0 线性增加到最大值（Peak LR）
 
  + 作用：防止模型在随机初始化阶段因为梯度过大而崩盘，帮助模型平稳进入 Loss 峡谷

+ Peak LR（峰值学习率）的经验值

  + 7B 级别模型：通常在 $3 \times 10^{-4}$ 到 $6 \times 10^{-4}$ 之
 
  + 更大的模型：随着参数量增加，Peak LR 需要相应减小（例如 70B 模型常设为 $1 \times 10^{-4}$ 到 $2 \times 10^{-4}$）。

+ WSD 调度器（DeepSeek 常用）

  + Warmup: 快速上升
 
  + Stable: 保持高学习率长时间运行
 
  + Decay: 在最后 10% 或 20% 的数据量阶段，快速衰减到零。这被证明能比传统 Cosine 获取更好的最终 Loss

**3. Batch Size 设置的 Trick**

+ 渐进式 Batch Size (Batch Size Ramp-up)：

  + 技巧：不要一开始就用 4M 或 8M 的 Global Batch Size。先从小 BS 开始（例如从 128 开始），随着训练步数逐步增加到目标 BS
 
  + 作用：在训练早期（模型还不稳定时）增加更新频率，在后期（需要更精确梯度方向时）增加单步样本量，能有效减少训练总时长

+ Micro Batch Size 与梯度累加 (Gradient Accumulation)

  + 技巧：单卡能跑多大就设多大（通常是 2 的幂次方，如 2, 4, 8），以榨干 GPU 计算核心（利用 Tensor Core 的对齐特性）
 
  + 计算公式：$\text{Global Batch Size} = \text{Micro Batch Size} \times \text{DP Size} \times \text{Accumulation Steps}$

**4. 其它实战经验建议**

+ Weight Decay (权重衰减)：通常固定在 0.1 左右。不要让它随 LR 缩放，保持稳定即可

+ Gradient Clipping (梯度裁剪)：通常设为 1.0。如果训练不稳定，可以尝试降到 0.5。如果经常触发裁剪，说明学习率可能设高了

+ Adafactor 或 AdamW：AdamW 是标配。注意 $\beta_2$ 的设置，对于超大规模训练，$\beta_2$ 设置为 0.95 或 0.98 有时比默认的 0.999 更能防止早期 Loss 震荡

总结：如何开始调优

1. 先定模型规模和数据量：根据 Scaling Law 估算所需的总 Tokens 数。

2. 选定 Batch Size：7B 模型通常选择 4M (4百万个 tokens) 作为全量训练的 BS。

3. 确定 Peak LR：参考同规模模型的论文（如 Llama 3 报告的值），先跑一个小比例的训练观察 Loss 是否下降顺滑。

4. 动态调整：如果 Loss 出现 Spikes（尖峰），调大 Warmup 步数或减小 Peak LR
