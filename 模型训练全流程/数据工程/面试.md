## 1. 预训练的文本被截断怎么办？

预训练数据（如 Common Crawl）通常非常长，远超模型单次处理的上下文窗口（Context Window，如 4096 或 8192）。

如果处理不当，会导致数据利用率低或丢失跨文档的语义信息。目前工业界主流的解决方案有以下三种：

**1. 拼接与分块 (Packing / Constant Length Padding)**

这是最常用的方法（如 Llama、GPT 系列）。不再把一个文档看作一个样本，而是将无数个文档连成一个**超长序列**。

+ 操作步骤：

  1. 将成千上万个文档的 Token 用特殊的 EOS（结束符）连接起来，形成一个无限长的 Token 流。
 
  2. 按照模型的最大长度（如 4096）进行等距离“截断”。
 
  3. 这样，每个训练样本都是满的。

+ 优点：计算效率极高，没有 PAD（填充），不会浪费算力。

+ 面试加分点

  由于一个样本中可能包含多个不相关的文档，为了防止模型学到错误的关联，通常会在 Attention 掩码中做处理（如 Document Masking），让模型在计算注意力时不能跨越 EOS。

**2. 滑动窗口采样 (Sliding Window / Overlap)**

如果某些长文档的连续性非常重要，可以采用带重叠的滑动窗口。

+ 操作步骤：

  1. 取前 $L$ 个 Token 作为第一个样本。
  
  2. 向后移动 $S$ 个步长（$S < L$），取下一个 $L$ 长度的块。

+ 优点：中间的部分会被模型多次看到，增强了对被截断处上下文的理解。

+ 缺点：计算开销增加（冗余计算），通常用于微调阶段（SFT）处理长文档，而非大规模预训练。

**3. 处理“断头断尾”的策略 (Special Tokens)**

当文本被截断时，必然会出现某个文档在样本开头是残缺的，或者在末尾被切断了。

+ FIM (Fill-In-the-Middle)：一些模型（如 CodeLlama）在预训练时会随机把中间一段切掉挪到最后，训练模型根据前后文预测中间。这增强了模型对截断边缘的推断能力。

+ Best-fit Packing：这是一种更高级的算法，类似于“装箱问题”。在构造 Batch 时，尽量挑选长度相加正好等于 $L$ 的几个文档放进一个样本，最大限度减少截断。

**4. 面试核心追问：截断对 Loss 的影响**

面试官可能会问：**“如果一个文档被切成两半放在两个 Batch 里，Loss 怎么算？”**

+ 回答要点：

  1. 状态不可跨越：在标准的 Transformer 训练中，两个 Batch 之间是独立的，模型在读第二半部分时，已经失去了第一半部分的隐藏状态（除非使用 Transformer-XL 这种具有记忆机制的架构，但目前主流大模型很少用）。

  2. 损失补偿：由于文档开头缺失上下文，模型预测前几个 Token 的 Loss 会异常高。通过 Packing 并在文档间加入 EOS，可以明确告诉模型“这里是新开始”，从而稳定 Loss。

| 方案 | 适用场景 | 核心优势 |
|-----|---------|------|
| Packing (拼接) | 大规模预训练 (Base Model) | 训练效率最高，无算力浪费 |
| Truncation (直接截断) | 简单微调 | 实现简单 |
| Sliding Window (滑动窗口) | 极长文本理解/代码生成 | 缓解边缘语义丢失 |
