## 预训练的 loss 如何计算？

目前绝大多数大模型（如 GPT 系列、Llama、DeepSeek）都属于**仅解码器（Decoder-only）** 架构，采用的是 **自回归语言建模（Autoregressive Language Modeling）**。

1. 核心任务：下一个 Token 预测

预训练的本质是让模型根据已有的上文预测下一个单词。

+ **输入序列**：假设有一个句子 $S = [w_1, w_2, \dots, w_n]$。

+ **目标**：在每一个位置 $t$，利用前 $t$ 个词预测第 $t+1$ 个词

2. 计算流程：从 Embedding 到 Loss

**步骤一：逻辑回归输出**

模型最后一层是一个线性层（LM Head），它将 Transformer 输出的隐藏向量 $h_t$ 映射到整个词表的大小 $V$。

对于每个位置 $t$，模型会生成一个长度为 $V$ 的向量 $z_t$，称为 Logits。

**步骤二：概率分布化**

使用 $Softmax$ 函数将 Logits 转换为概率分布 $P$：

$$P(w_{t+1}|w_{1:t})=Softmax(z_t)=frac{exp(z_{t,i})}{\sum_{j=1}^{V}exp(z_{t,j})}$$

**步骤三：交叉熵损失**

模型的目标是让对应“真实标签”那个词的概率最大化。我们使用 负对数似然，即交叉熵损失：

对于单个 Token 的损失：

$$\ell_t = -\log P(x_{t+1} = \text{target})$$

对于长度为 $N$ 的整个序列，总Loss 是所有有效位置的平均值：

$$\mathcal{L} = -\frac{1}{N-1} \sum_{t=1}^{N-1} \log P(w_{t+1} | w_{1:t})$$

### 加分技术细节

**A. 因果掩码**

在计算过程中，为了防止模型“偷看”答案，Transformer 使用了下三角矩阵（Mask）。这确保了在计算第 $t$ 个位置的 Loss 时，注意力机制只能看到 $1 \dots t$ 的信息。

**B. 标签偏移**

这是代码实现中最容易出错的地方。在训练时：

+ **Input**: $[Token_1, Token_2, Token_3]$

+ **Target**: $[Token_2, Token_3, Token_4]$ 你需要将输入序列向右移一位，或者将标签序列向左移一位，以对齐“预测值”和“真实值”

**C. 忽略填充**

为了批量处理（Batching），短句子会被填充 PAD 字符。在计算 Loss 时，必须设置 ignore_index（通常在 PyTorch 中设为 -100），使 PAD 字符不参与梯度计算。

**D. 算子融合**

在大模型训练中，由于词表 $V$ 非常大（如 128k），直接计算 Cross Entropy 会产生巨大的显存占用。现代框架（如 FlashAttention 或 Unsloth）会使用算子融合技术，直接在 CUDA 核中计算 Loss 并不存储完整的 Logits 矩阵。

## SFT 的 loss 如何计算？

**1. 核心逻辑：只计算 Answer 部分的 Loss**

SFT 的数据通常由 Prompt（提示词/问题） 和 Answer（答案/回复） 两部分组成。

+ **预训练 (Pre-train)**：对序列中的每一个 Token 都计算 Loss。

+ **SFT**：虽然整个 Prompt + Answer 都会输入模型，但**模型只对 Answer 部分的 Token 产生梯度并计算 Loss**。Prompt 部分仅作为上下文（Context），不参与 Loss 的计算。

**2. 数学表达式**

假设输入序列 $X$ 由两部分拼接而成： $X = [x_1, \dots, x_m, x_{m+1}, \dots, x_n]$。其中 $x_1 \dots x_m$ 是 Prompt， $x_{m+1} \dots x_n$ 是 Answer。

SFT 的损失函数 $\mathcal{L}$ 为：

$$\mathcal{L} = -\frac{1}{n-m} \sum_{t=m}^{n-1} \log P(x_{t+1} | x_{1:t})$$

这里的关键在于求和符号是从 $m$（即 Prompt 结束的位置）开始的。

**3. SFT Loss 计算的具体步骤**

**第一步：序列拼接 (Concatenation)**

将 Prompt 和 Answer 拼接在一起。通常会在中间加入特殊的各种分隔符（如 ChatML 格式中的 <|im_start|>assistant）。

**第二步：构建掩码 -- 最核心步骤**

为了实现“只计算 Answer 部分”，我们需要构建一个与输入等长的**Labels 序列**：

+ **Prompt 对应位置**：在 Labels 中填充一个特殊值（在 PyTorch 中通常是 -100）。

+ **Answer 对应位置**：保留原始的 Token ID。

**第三步：前向传播与交叉熵**

模型照常进行前向传播得到 Logits。但在计算交叉熵损失（Cross-Entropy Loss）时，设置 ignore_index=-100。

**原理**：PyTorch 的 ```CrossEntropyLoss``` 算子如果遇到标签值为 -100 的位置，会自动跳过该位置的梯度计算和 Loss 累加。

**4. 面试加分项：为什么不计算 Prompt 的 Loss？**

1. **学习重点不同**：SFT 的目的是让模型学习“如何遵循指令”并输出“正确的回答”。Prompt 是给定的已知信息，模型不需要学习如何预测 Prompt。

2. **防止灾难性遗忘**：如果计算 Prompt 部分的 Loss，模型会倾向于去背诵这些特定的问题模板，而不是学习回答问题的逻辑，这可能导致模型在处理未见过的问题时泛化能力下降。

3. **计算效率**：忽略掉大量 Prompt 的梯度计算可以节省一定的资源，虽然在现有的框架下这部分节省有限，但在长文本 SFT 中意义重大。
