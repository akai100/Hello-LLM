## 预训练的 loss 如何计算？

目前绝大多数大模型（如 GPT 系列、Llama、DeepSeek）都属于**仅解码器（Decoder-only）** 架构，采用的是 **自回归语言建模（Autoregressive Language Modeling）**。

1. 核心任务：下一个 Token 预测

预训练的本质是让模型根据已有的上文预测下一个单词。

+ **输入序列**：假设有一个句子 $S = [w_1, w_2, \dots, w_n]$。

+ **目标**：在每一个位置 $t$，利用前 $t$ 个词预测第 $t+1$ 个词

2. 计算流程：从 Embedding 到 Loss

**步骤一：逻辑回归输出**

模型最后一层是一个线性层（LM Head），它将 Transformer 输出的隐藏向量 $h_t$ 映射到整个词表的大小 $V$。

对于每个位置 $t$，模型会生成一个长度为 $V$ 的向量 $z_t$，称为 Logits。

**步骤二：概率分布化**

使用 $Softmax$ 函数将 Logits 转换为概率分布 $P$：

$$P(w_{t+1}|w_{1:t})=Softmax(z_t)=frac{exp(z_{t,i})}{\sum_{j=1}^{V}exp(z_{t,j})}$$

**步骤三：交叉熵损失**

模型的目标是让对应“真实标签”那个词的概率最大化。我们使用 负对数似然，即交叉熵损失：

对于单个 Token 的损失：

$$\ell_t = -\log P(x_{t+1} = \text{target})$$

对于长度为 $N$ 的整个序列，总Loss 是所有有效位置的平均值：

$$\mathcal{L} = -\frac{1}{N-1} \sum_{t=1}^{N-1} \log P(w_{t+1} | w_{1:t})$$

### 加分技术细节

**A. 因果掩码**

在计算过程中，为了防止模型“偷看”答案，Transformer 使用了下三角矩阵（Mask）。这确保了在计算第 $t$ 个位置的 Loss 时，注意力机制只能看到 $1 \dots t$ 的信息。

**B. 标签偏移**

这是代码实现中最容易出错的地方。在训练时：

+ **Input**: $[Token_1, Token_2, Token_3]$

+ **Target**: $[Token_2, Token_3, Token_4]$ 你需要将输入序列向右移一位，或者将标签序列向左移一位，以对齐“预测值”和“真实值”

**C. 忽略填充**

为了批量处理（Batching），短句子会被填充 PAD 字符。在计算 Loss 时，必须设置 ignore_index（通常在 PyTorch 中设为 -100），使 PAD 字符不参与梯度计算。

**D. 算子融合**

在大模型训练中，由于词表 $V$ 非常大（如 128k），直接计算 Cross Entropy 会产生巨大的显存占用。现代框架（如 FlashAttention 或 Unsloth）会使用算子融合技术，直接在 CUDA 核中计算 Loss 并不存储完整的 Logits 矩阵。
