## 1. 为什么深度神经网络需要参数初始化？

深度神经网络需要参数初始化主要是为了保证训练的稳定性和有效性。如果参数初始化不当，信号在前向传播时可能会逐层缩小或放大，导致梯度消失或梯度爆炸，从而网络无法收敛。
此外，如果权重全为相同值（如全 0），所有神经元会学习相同的特征，模型丧失表达能力。因此，参数初始化的目标是打破对称性并保持前向输出和反向梯度的方差稳定，这样可以让网络更快更稳定地收敛。

## 2. 如果全 0 初始化会发生什么？

## 3. Xavier 初始化和 He 初始化有什么区别？

## 4. 方差为什么要与 fan_in/fan_out 相关？

因为前向/反向传播时，方差会乘层数，保持方差不变可防止梯度爆炸或消失.

## 5. 训练深层网络梯度一直消失，你会怎么排查？

## 6. 如果 Xavier 初始化失败，改成 He 会有帮助吗？

## 7. 大模型（如 BERT/GPT）Embedding 层如何初始化？

## 8. Transformer 的 linear 和 attention 权重怎么初始化？

## 9. 多任务学习中，不同任务共享的参数初始化策略是什么？
