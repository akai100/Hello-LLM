## 1. 为什么深度神经网络需要参数初始化？

## 2. 如果全 0 初始化会发生什么？

## 3. Xavier 初始化和 He 初始化有什么区别？

## 4. 方差为什么要与 fan_in/fan_out 相关？

因为前向/反向传播时，方差会乘层数，保持方差不变可防止梯度爆炸或消失.

## 5. 训练深层网络梯度一直消失，你会怎么排查？

## 6. 如果 Xavier 初始化失败，改成 He 会有帮助吗？

## 7. 大模型（如 BERT/GPT）Embedding 层如何初始化？

## 8. Transformer 的 linear 和 attention 权重怎么初始化？

## 9. 多任务学习中，不同任务共享的参数初始化策略是什么？
