## 1. 为什么深度神经网络需要参数初始化？

深度神经网络需要参数初始化主要是为了保证训练的**稳定性**和**有效性**。如果参数初始化不当，信号在前向传播时可能会逐层缩小或放大，导致梯度消失或梯度爆炸，从而网络无法收敛。
此外，如果权重全为相同值（如全 0），所有神经元会学习相同的特征，模型丧失表达能力。因此，参数初始化的目标是打破对称性并保持前向输出和反向梯度的方差稳定，这样可以让网络更快更稳定地收敛。

## 2. 如果全 0 初始化会发生什么？

## 3. Xavier 初始化和 He 初始化有什么区别？

## 4. 方差为什么要与 fan_in/fan_out 相关？

因为前向/反向传播时，方差会乘层数，保持方差不变可防止梯度爆炸或消失.

## 5. 训练深层网络梯度一直消失，你会怎么排查？

1. 观察训练日志

   + loss 长期不下降 -> 可能梯度消失
   + 梯度范数（norm）接近 0

2. 检查梯度分布

   + 对每层梯度做统计（mean/std）
   + 是否在深层几乎为 0

3. 检查激活分布

   + 前向传播输出是否在饱和区（sigmoid/tanh 的 ±1）
   + 是否过小 → 梯度衰减

4. 检查权重初始化

  + 权重是否过小 → 方差过小 → 梯度被压缩
  + 是否全 0 → 对称性无法打破

5. 检查网路结构

   + 是否深层没有残差连接
   + 是否使用了不合适的激活函数
   

## 6. 如果 Xavier 初始化失败，改成 He 会有帮助吗？

## 7. 大模型（如 BERT/GPT）Embedding 层如何初始化？

## 8. Transformer 的 linear 和 attention 权重怎么初始化？

## 9. 多任务学习中，不同任务共享的参数初始化策略是什么？
