
He 初始化是一种专门针对 ReLU 及其变体的权重初始化方法，目的是保持前向和反向传播的方差稳定，避免梯度消失或爆炸。

## 解决问题

传统随机初始化（如标准正态分布）在深层网络中会导致：

1. **前向方差爆炸或消失** → 激活值变太大或太小

2. **梯度方差爆炸或消失** → 导致训练困难

尤其是 **ReLU 激活**，一半输出为 0，直接影响方差。

## 数学公式

### 基本原理

假设一层线性变换：

$$y=Wx$$

+ 输出维度： $n$

+ 我们希望 **输出方差 $\approx$ 输入方差**

### 对于 ReLU 激活

+ ReLU 会把负值置 0 → 方差减半

+ 为了补偿，He 初始化选择：

均值为 0，方差为

$$Var(W)=\frac{2}{n_{in}}$$

其中：

+ $n_{in} = $ 前一层输入神经元数
+ 对于 Leaky ReLU，可乘上 $\frac{1}{1+\alpha^2}$
