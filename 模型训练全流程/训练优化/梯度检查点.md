
## 1. 什么是梯度检查点？

梯度检查点是一种「以计算换显存」的优化策略 —— 正常训练时会**保存所有层的中间激活值**（用于反向传播算梯度），而梯度检查点只会**选择性保存少量关键层的激活值**（称为「检查点」），
反向传播时再重新计算其他层的激活值，从而大幅降低激活值的显存占用。

核心目标：针对模型层数极深（如 BERT-large、GPT-2）导致的激活值显存爆炸问题，进一步压缩显存占用（梯度累积是拆 batch，梯度检查点是拆模型层）。

## 2. 为什么能省显存？

### 2.1. 正常反向传播的激活值存储

以 12 层 BERT 为例，前向传播时的显存逻辑：

```
步骤1：输入层 → 第1层 → 保存第1层激活值 → 第2层 → 保存第2层激活值 → ... → 第12层 → 保存第12层激活值；
步骤2：反向传播时，从第12层往第1层算梯度，每算一层都需要用该层的激活值（依赖前向保存的所有激活值）；
```

### 2.2 梯度检查点的核心逻辑

梯度检查点把模型拆成若干「段」，只保存每段的**输入 / 输出（检查点）**，反向时重新计算段内的激活值：

```
# 前向传播（以12层BERT拆成3段，每段4层为例）
步骤1：输入层 → 第1-4层 → 只保存「第1层输入」和「第4层输出」（检查点），不保存1-4层中间激活值 → 释放1-4层临时显存；
步骤2：第5-8层 → 只保存「第5层输入」和「第8层输出」（检查点） → 释放5-8层临时显存；
步骤3：第9-12层 → 保存「第9层输入」和「第12层输出」（检查点） → 释放9-12层临时显存；

# 反向传播
步骤1：从第12层往第9层算梯度 → 先**重新计算**9-12层的激活值（用检查点的输入/输出）→ 算完梯度后立刻释放这些激活值；
步骤2：从第8层往第5层算梯度 → 重新计算5-8层的激活值 → 算完释放；
步骤3：从第4层往第1层算梯度 → 重新计算1-4层的激活值 → 算完释放；
```
