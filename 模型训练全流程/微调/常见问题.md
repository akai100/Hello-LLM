## 案例一：SFT 后效果“线下很好，线上大崩”

### 现象

+ 离线评测
  + BELU / Rouge / 人工测评都 OK
+ 上线后：
  + 回答变短
  + 频繁拒答
  + “泛化能力明显下降”
+ PM 反馈：“模型变笨了”

### 根因分析

#### 1️⃣ 指令数据分布过窄（Instruction Overfitting）

SFT 数据：
+ 90% 是短指令
+ 模板高度一致
+ 几乎没有复杂上下文、多轮对话
结果：
+ 模型学会了“怎么按模板答”
+ 忘了 pretrain 阶段学到的泛化能力

#### 2️⃣ 学习率过大，破坏 base model 能力

典型配置：

## 案例二：微调后线上推理 OOM，训练时却没问题

现象

微调完成，loss 正常

上线推理：

并发一上来就 OOM

显存使用比 base model 高很多

根因分析
1️⃣ LoRA + 长上下文 = KV Cache 爆炸

微调数据：

max_seq_len = 4096

大量长样本

结果：

模型学会生成更长回答

平均输出 token 数 ↑↑

👉 推理时 KV Cache 使用量飙升

2️⃣ 推理参数没限制

max_new_tokens = 2048

没有 early stop

没有 repetition penalty

定位方式

对比：

SFT 前后：

平均 prompt 长度

平均生成长度

监控：

KV Cache 占用

TPS 突降点

解决方案
✅ 推理侧

限制：

max_new_tokens = 512


加 stop words

设置 repetition penalty

✅ 训练侧

混入 “短答样本”

显式教模型 “何时停止”

可复用经验

微调会改变“生成长度分布”，而推理系统最怕的就是长度失控
