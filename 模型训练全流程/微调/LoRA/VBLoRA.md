## 核心思想

**VBLoRA = 给 LoRA 的低秩参数“加上不确定性”，用变分贝叶斯而不是点估计来学习它们。**

它解决的是：**LoRA 只学一个最优权重值，但不知道这个权重“有多可靠”。**

## 从 LoRA 到 VBLoRA 的演进

### 1️⃣ LoRA 回顾（确定性）

对于原始权重矩阵 $W \in R^{d \times k}$:

$$W'=W + \Delta W$$

LoRA 约束：

$$\Delta W = BA, A \in R^{r \times k}, B \in R^{d \times r}$$

训练时：

+ $W$ 冻结

+ 只优化 $A, B$

+ 点估计

### 2️⃣ VBLoRA 的关键变化（随机变量）

VBLoRA 不再把 $A, B$当作确定值，而是当作随机变量：

$$A \text{∼} q(A), B ∼ q(B)$$

通常假设：

$$q(A)=N(u_A, \sigma_{A}^{2}), q(B)=N(u_B, \sigma_{B}^2)$$

于是：

$$\Delta W = BA 是随机的$$

## 数学原理

### 1️⃣ 目标：后验分布

我们希望得到：

$$p(A, B | D)$$

但真实后验不可解 -> 变分推断（VI）

### 2️⃣ 变分近似

用简单分布 $q(A, B)$近似真实后验：

$$q(A, B) \approx p(A, B | D)$$

通过最小化 KL:

$$KL(q(A, B)||p(A, B | D))$$

等价于最大化 ELBO:

$$L_{ELBO}=E_q(A,B)[log p(D|A,B)]-KL(q(A,B)||p(A,B))$$

### 3️⃣ ELBO 的两部分

(1) 数据项（似然）

```
模型预测要拟合数据
```

$$E_{q(A,B)}[logp(y|f(x;W+BA))]$$

通过 **Monte Carlo 采样** 估计

(2) 正则项（KL）

```
让 LoRA 参数不要偏离先验
```

通常设：

$$p(A),p(B)=N(0,\sigma_{0}^2)$$

KL 有解析解（高斯 Vs 高斯）

## 实现流程（工程视角）

### 1️⃣ 参数化方式（最关键）

用 重参数化技巧：

$$A = u_A+\sigma_A \odot \epsilon, \epsilon ∼ N(0, 1)$$

同理：

$$B = u_B + \sigma_B \odot \epsilon$$

训练的参数是：

+ $u_A, log \sigma_A$

+ $u_B, log \sigma_B$

## 优势

**✅ 不确定性建模**

+ 可输出预测方差

+ 适合安全/医疗/金融

**✅ 自动正则化**

KL 相当于自适应 weight decay

**✅ 抗过拟合**

+ 小数据微调效果更稳

## 代价与挑战

| 问题   | 原因          |
| ---- | ----------- |
| 训练慢  | 采样 + KL     |
| 显存翻倍 | 均值 + 方差     |
| 推理复杂 | 需要采样 / 均值近似 |


## 推理阶段怎么用

1️⃣ MAP 推理（最快）
​
$$A=u_A, B=u_B$$

2️⃣ MC Sampling

+ 多次采样 → 均值 + 方差

3️⃣ 不确定性筛选

+ 高方差输出拒绝预测

## VBLoRA vs LoRA 总结

| 维度    | LoRA | VBLoRA |
| ----- | ---- | ------ |
| 参数    | 确定   | 分布     |
| 训练    | SGD  | VI     |
| 不确定性  | ❌    | ✅      |
| 小样本   | 一般   | 更稳     |
| 工程复杂度 | 低    | 高      |
