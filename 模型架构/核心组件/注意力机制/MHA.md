
## 1. 什么是 MHA?

**多头注意力（Multi-Head Attention）** 是 Transformer 的核心机制，它通过**并行的多个注意力头**，让模型在**不同子空间**、**不同位置关系**上同时关注信息，从而更好地建模序列依赖关系。
