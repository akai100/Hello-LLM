# 1. Transformer 模型

Transformer 模型是由谷歌在2017年提出并首先用于机器翻译的神经网络模型架构。

Transformer 结构完全通过注意力机制完成对源语言序列和目标序列全局依赖的建模。


# 2. 嵌入表示层

对于输入文本序列，首先通过输入嵌入层（Input Embedding）将每个单词转换为其相对应的向量表示。在送入编码器端建模其上下文语义之前，一个
非常重要的操作是在词嵌入中加入**位置编码（Position Encoding）** 这一特征。

具体来说，序列中每一个单词所在的位置都对应一个向量，这一向量会与单词表示对应相加并送入到后续模块中做进一步处理。

为了得到不同位置对应的编码，Transformer 模型使用不同频率的正余弦函数：

 $PE(pos, 2i)=sin(\frac{pos}{10000^{2i/d}})$

 $PE(pos, 2i+1)=cos(\frac{pos}{10000^{2i/d}})$

其中

+ $pos$ 表示单词所在的位置

+ $2i$ 和 $2i+1$ 表示编码向量中的对应维度

+ $d$ 对应位置编码的总维度

好处：

+ 正余弦函数的范围是在 [-1, +1]，导出的位置编码与原词嵌入相加不会使得结果偏离过远而破坏原有单词的语义信息；

+ 依据三角函数的基本性质，可以得知第 $pos + k$ 个位置的编码是第 $pos$ 个位置的编码的线性组合，这就意味着位置编码中蕴含着单词之间的距离信息；

## 2.1 举例说明

（1）假设序列最大长度 max_len = 3；

（2）嵌入维度 d_model = 4；

**步骤 1：逐位置、逐维度计算**

**步骤 1.1：位置 0（第一个 token）**

+ 维度 0： $PE(0, 0)=sin(0/10^0)=sin(0)=0$

+ 维度 1： $PE(0, 1)=cos(0/10^0)=cos(0)=1$

+ 维度 2： $PE(0, 2)=sin(0/10^1)=sin(0)=0$

+ 维度 3： $PE(0, 3)=cos(0/10^1)=cos(0)=1$

  位置 0 的编码： $[0, 1, 0, 1]$

**步骤 1.2：位置 1 （第二个 token）**

+ 维度 0： $PE(1,0) = \sin(1 / 10^0) = \sin(1) ≈ 0.8415$

+ 维度 1： $PE(1,1) = \cos(1 / 10^0) = \cos(1) ≈ 0.5403$

+ 维度 2： $PE(1,2) = \sin(1 / 10^1) = \sin(0.1) ≈ 0.0998$

+ 维度 3： $PE(1,3) = \cos(1 / 10^1) = \cos(0.1) ≈ 0.9950$

 位置 1 的编码： $[0.8415, 0.5403, 0.0998, 0.9950]$

 **步骤 1.3：位置 2 （第三个 token）**

+ 维度 0： $PE(2,0) = \sin(2 / 10^0) = \sin(2) ≈ 0.9093$

+ 维度 1： $PE(2,1) = \cos(2 / 10^0) = \cos(2) ≈ -0.4161$

+ 维度 2： $PE(2,2) = \sin(2 / 10^1) = \sin(0.2) ≈ 0.1987$

+ 维度 3： $PE(2,3) = \cos(2 / 10^1) = \cos(0.2) ≈ 0.9801$

   位置 2 的编码： $[0.9093, -0.4161, 0.1987, 0.9801]$

**步骤 2：结合 token 嵌入使用**

+ 位置 0 的 token（如 "我"）： $[0.1, 0.2, 0.3, 0.4]$

+ 位置 1 的 token（如 "爱"）： $[0.5, 0.6, 0.7, 0.8]$

+ 位置 2 的 token（如 "你"）： $[0.9, 1.0, 1.1, 1.2]$


位置编码与 token 嵌入相加

+ 位置 0： $[0.1, 0.2, 0.3, 0.4] + [0, 1, 0, 1] = [0.1, 1.2, 0.3, 1.4]$

+ 位置 1： $[0.5, 0.6, 0.7, 0.8] + [0.8415, 0.5403, 0.0998, 0.9950] = [1.3415, 1.1403, 0.7998, 1.7950]$

+ 位置 2： $[0.9, 1.0, 1.1, 1.2] + [0.9093, -0.4161, 0.1987, 0.9801] = [1.8093, 0.5839, 1.2987, 2.1801]$

**解释**

+ 唯一性

  每个位置的编码向量完全不同（位置 0 全是 0/1，位置 1 是中值，位置 2 维度 1 为 负），模型能区分不同位置；

+ 相邻位置相似性

  位置 0→1→2 的编码变化是连续的（维度 2/3 从 0 缓慢增加），符合 “相邻 token 位置关联强” 的直觉；

+ 维度分层

  + 低维度（0/1）：周期短（10^0=1），值变化快（位置 0→2 从 0→0.9093→-0.4161），捕捉近距离位置差异；
 
  + 高维度（2/3）：周期长（10^1=10），值变化慢（位置 0→2 从 0→0.1987→0.9801），捕捉远距离位置差异；

+ 泛化性

  即使序列长度超过 3（比如位置 3），仍可按公式计算

**对比“无位置编码”的问题**

如果没有位置编码，模型看到的输入是：
+ “我”：[0.1, 0.2, 0.3, 0.4]
  
+ “爱”：[0.5, 0.6, 0.7, 0.8]
  
+ “你”：[0.9, 1.0, 1.1, 1.2]

无法区分"我爱你" 和 “你爱我”（token 嵌入顺序反转，模型无感知）

加上位置编码后，顺序反转的输入向量完全不同，模型能识别语序差异


# 3. 注意力层

自注意力（Self-Attension）操作用以建模源语言、目标语言任意两个单词之间的依赖关系。

给定由单词语义嵌入及其位置编码叠加得到的输入表示 $\{x_i \in R^{d}\}_{i=1}^{t}$

为了实现对上下文语义依赖的建模，进一步引入在自注意力机制中涉及到的三个元素：

+ 查询 $q_i(Query)$

+ 键 $k_i(Key)$

+ 值 $v_i(value)$

在编码输入序列中每一个单词的表示的过程中，这三个元素用于计算上下文单词所对应的权重得分。直观来说，这些权重反映了在编码当前单词的表示时，
对于上下文不同部分所需要的关注程度。

具体来说，通过三个线性变换 $W^Q \in R^{d * d_q}$, $W^K \in R^{d * d_k}$， $W^V \in R^{d * d_v}$，将输入序列中的每一个单词表示 $x_i$ 转换
为其对应的 $q_i \in R^{d_k}$， $k_i \in R_{d_k}$， $v_i \in R^{d_v}$ 向量。


为了得到编码单词 $x_i$ 时所需要关注的上下文信息，通过位置 $i$ 查询向量与其他位置的键向量做点积得到匹配分数 $q_i \dot k_1,q_i \dot k_2,...,q_i \dot k_t$。

为了防止过大的匹配分数在后续 softmax 计算过程中导致的梯度爆炸以及收敛效率差的问题，这些得分会除缩放因子 $\sqrt{d}$ 以稳定优化。

缩放后的得分经过 Softmax 归一化为概率之后，与其他位置的值向量相乘来聚合希望关注的上下文信息，并最小化不相关信息的干扰。上述计算过程形式化表示：

$Z=Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d}})V$

$Z$ 表示自注意力操作的输出，为了进一步增强自注意力机制聚合上下文信息的能力，提出了**多头自注意力**的机制，以关注上下文的不同侧面。

具体来说，上下文中每一个单词的表示 $x_i$ 经过多组线性 $\{W_{j}^{Q}W_{j}^{K}W_{j}^{V}\}_{j=1}^{N}$ 映射不到的表示子空间中。

# 4. 前馈层

前馈层接收自注意力子层的输出作为输入，并通过一个带有 Relu 激活函数的两层全连接网络对输入进行更加复杂的非线性变换。

$FFN(x)=Relu(xW_1+b+1)W_2+b_2$

实验结果表明，增大前馈子层隐状态的维度有利于提升最终翻译结果的质量。

## 4.1 残差连接与层归一化

编码器和解码器均由很多层基本的 Transformer 块组成，每一层当中都包含复杂的非线性映射，这就导致模型的训练比较困难。

因此，研究者们在Transformer 块中进一步引入了**残差连接**与**层归一化技术**以进一步**提升训练的稳定性**。

残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出上去，从而避免由于网络过深在优化过程中潜在的**梯度消失**问题：

 $x^{l+1}=f(x^l)+x^l$

其中， $x^l$ 表示第 $l$ 层的输入， $f(\dot)$ 表示一个映射函数。

此外，为了进一步使得每一层的输入输出范围稳定在一个合理的范围内，层归一化技术被进一步引入每个 Transformer 快：

 $LN(x)=\alpha \dot \frac{x-μ}{σ} + b$

其中，μ 和 σ 分别表示均值和方差，用于将数据平移缩放到均值为0，方差为1的标准分布。 $\alpha$ 和  $b$ 是可学习的参数。

层归一化技术可以有效地缓解优化过程中潜在的不稳定、收敛速度慢等问题。

# 4.2 编码器和解码器结构
