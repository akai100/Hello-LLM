
GLU 是一种带“门控机制”的激活结构，通过一个门来选择性地让信息通过，而不是简单地对每个维度做非线性变换。

它本质上是：

**线性变换 + 门控选择**

## GLU 的提出背景

传统激活函数（ReLU/GELU）的问题：

+ 所有维度一视同仁

+ 没有“是否重要”的显示判断


## 数学定义

$$GLU(x)=(W_1x+b_1)\odot \sigma(W_xx+b_2)$$

其中：

+ $W_1x：信息通道

+ $W_2x$：门控通道

+ $\sigma$：sigmoid

+ $\odot$：逐元素乘法

**含义：**

门的值 $\in (0,1)$，决定信息保留多少

## 实现

**传统 FFN (以 GELU 为例)**

```
x → Linear(d → 4d) → GELU → Linear(4d → d)
```

**GLU FFN**

```
x ──→ Linear1(d → 2d) ─┐
                        ⊙ → Linear3(2d → d)
x ──→ Linear2(d → 2d) ─┘ (gate)
```

## 优点

### 1️⃣ 表达能力更强（参数更“值钱”）

+ 门控机制

  + 允许模型**动态选择特征**
+ 同等参数下：

  + 表达能力 > GELU

### 2️⃣ 梯度更稳定

+ 门控是**平滑函数**

+ 避免 ReLU硬截断

+ 长训练不容易数值爆炸
