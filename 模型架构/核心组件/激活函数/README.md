## 传统激活函数
### Sigmoid(logistic)

**定义：**

$$\sigma(x)=\frac{1}{1+e^{-x}}$$

输出范围：(0, 1)

**优点：**

+ 有概率意义

+ 平滑可导

+ 早期神经网络、逻辑回归常用

**缺点（重点）：**

+ 梯度消失

+ 输出非零中心（mean ≠ 0）

+ 计算开销大

### Tanh(双曲正切)

**定义：**

$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$

输出范围：(-1, 1)

**优点：**

+ 零中心（比 sigmoid 好）

+ 梯度更大

**缺点：**

+ 仍然存在梯度消失

+ 大 $|x|$区域饱和

**典型使用场景：**

+ 早期 RNN/LSTM 隐状态

### ReLU（Rectified Linear Unit）

**定义：**

$$f(x)=max(0,x)$$

输出范围：[0, +∞)

**优点：**

+ 计算及其简单

+ 缓解梯度消失

+ 稀疏激活（很多0）

**缺点：**

+ Dying ReLU 问题

  Dying ReLU 指的是在训练过程中，某些 ReLU 神经元的输入长期为负，输出恒为 0，梯度也恒为 0，从而神经元永久“死亡”，不再参与学习。

+ 非零中心

+ 对负值完全“掐死”

**典型使用场景:**

+ CNN/MLP 隐藏层（很长时间的默认选择）

### Leaky ReLU

**定义：**

$$
f(x) =
\begin{cases}
x, & x > 0 \\
\alpha x, & x \leq 0
\end{cases}
$$

+ 解决 Dying ReLU

+ $\alpha$ 通常是 0.01

### PReLU

+ $\alpha$可学习

+ 增加模型自由度

+ 常见于 CV 模型

### ELU

**定义：**

$$
f(x) =
\begin{cases}
x, & x > 0 \\
\alpha(e^x - 1), & x \leq 0
\end{cases}
$$

+ 输出更接近零均值

+ 计算稍复杂

### 对比

| 激活函数       | 输出范围    | 是否零中心 | 梯度消失 | 主要问题       |
| ---------- | ------- | ----- | ---- | ---------- |
| Sigmoid    | (0,1)   | ❌     | 严重   | 饱和         |
| Tanh       | (-1,1)  | ✅     | 有    | 饱和         |
| ReLU       | [0,+∞)  | ❌     | 缓解   | Dying ReLU |
| Leaky ReLU | (-∞,+∞) | ❌     | 缓解   | α 固定       |
| PReLU      | (-∞,+∞) | ❌     | 缓解   | 参数增多       |
| ELU        | (-α,+∞) | 接近    | 缓解   | 计算复杂       |


## GELU

**解决问题**

解决**传统激活函数在深层网络训练中存在的梯度消失、死亡神经元和表达能力不足**的核心问题。
