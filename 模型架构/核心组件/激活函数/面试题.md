## 1. Transformer / LLM 中常用的激活函数是什么？

**核心答案**

+ GELU（主流）

+ Swish（部分模型）

**模型示例**

+ BERT / GPT → GELU
+ PaLM → Swish

## 2. GELU 的直觉和公式？

$$GELU(x)=x\cdot \Phi(x)$$

**直觉理解**

+ 不是“硬截断”，而是“概率性保留”
+ 小负值仍可能被保留

**加分解释**

+ 比 ReLU 平滑
+ 更符合高斯分布假设

## 3. 为什么 GELU 比 ReLU 更适合大模型？

**面试官期待**

+ 梯度更平滑
+ 对噪声鲁棒
+ 收敛更稳定
+ 对 LayerNorm 友好

## 4. Swish 和 GELU 的区别

$$Swish(xx\cdot \sigma(x)$$

| 维度    | GELU | Swish |
| ----- | ---- | ----- |
| 理论基础  | 概率解释 | 经验    |
| 平滑性   | 高    | 高     |
| 计算复杂度 | 略高   | 略低    |

## 5. 激活函数如何影响梯度消失 / 爆炸？

**关键点**
+ 激活函数导数范围
+ 是否存在饱和区
+ 是否 zero-gradient 区域

## 6. 激活函数如何影响初始化策略？

**示例**

+ ReLU → He 初始化

+ Tanh → Xavier 初始化

**追问**

GELU 对初始化有什么要求？

## 7. LayerNorm + GELU 为什么是 Transformer 标配？

**标准回答**

+ LayerNorm 保证输入分布稳定

+ GELU 提供平滑非线性

+ 二者组合提升深层稳定性

## 8. 激活函数在 LoRA / 微调中是否需要调整？

**加分点**

+ 通常不改
+ 但激活分布会影响：
  + 学习率
  + Adapter 插入位置
  + 收敛速度

## 9. 
