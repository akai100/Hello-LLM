
**旋转位置编码（Rotary Position Embedding，RoPE）** 是一种用于 Transformer 的相对位置编码方法，常见于 GPT-NeoX、LLaMA、Qwen 等模型。

直观理解：RoPE 把**位置信息通过“旋转”注入到查询 Q 和键 K 的向量空间中**，使得注意力分数天然地只依赖于**相对位置差**。

## 核心思想

对每个 token 的 Q、K 向量按维度成对做二维旋转，旋转角度由位置决定：

+ 每一对维度看成一个二维向量

+ 不同位置 -> 不同旋转角度

+ 两个 token 的点积结果 -> 自动包含相对位置信息

## 数学形式

设隐藏维度为 $d$ (偶数)，位置为 $p$：

1. 定义角频率：

$$\theta_i = 10000^{-2i/d}$$

2. 对第 $i$ 对维度做旋转：

$$
\begin{pmatrix}
x_{2i}' \\
x_{2i+1}'
\end{pmatrix}
= \begin{pmatrix}
\cos(p\theta_i) & -\sin(p\theta_i) \\
\sin(p\theta_i) & \cos(p\theta_i)
\end{pmatrix}
\begin{pmatrix}
x_{2i} \\
x_{2i+1}
\end{pmatrix}
$$

这个旋转 只作用在 Q 和 K 上，不作用在 V 上。

### 为什么它是“相对位置编码”

关键性质：

$$\<R(p)q, R(k)k\>=<q, R(k-p)k>$$

### 和其他位置编码的对比

| 方法                         | 类型     | 特点         |
| -------------------------- | ------ | ---------- |
| 绝对位置编码（Sinusoidal）         | 绝对     | 简单，但外推能力有限 |
| Learned Position Embedding | 绝对     | 参数化，不能外推   |
| Relative Bias（T5）          | 相对     | 需额外 bias 表 |
| **RoPE**                   | **相对** | 无额外参数、外推性好 |

### 优点

+ ✅ 无额外可学习参数

+ ✅ 长上下文外推能力强

+ ✅ 与标准注意力机制完全兼容

+ ✅ 计算高效（只是旋转）

### 局限与改进

+ 原始 RoPE 在极长上下文下仍会退化

+ 因此出现了改进版本：

  + NTK-aware RoPE

  + XPos

  + YaRN

  + Dynamic RoPE scaling
