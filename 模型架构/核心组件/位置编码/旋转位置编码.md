
**旋转位置编码（Rotary Position Embedding，RoPE）** 是一种用于 Transformer 的相对位置编码方法，常见于 GPT-NeoX、LLaMA、Qwen 等模型。

直观理解：RoPE 把**位置信息通过“旋转”注入到查询 Q 和键 K 的向量空间中**，使得注意力分数天然地只依赖于**相对位置差**。

## 核心思想

对每个 token 的 Q、K 向量按维度成对做二维旋转，旋转角度由位置决定：

+ 每一对维度看成一个二维向量

+ 不同位置 -> 不同旋转角度

+ 两个 token 的点积结果 -> 自动包含相对位置信息

1. 位置信息 = 旋转角度：给每个位置 $pos$ 分配一个 旋转角度 $\theta_{pos}$，位置越远，旋转角度越大，通过**角速度**区分不同维度的旋转速度；

2. 乘性旋转注入位置：不直接加位置嵌入，而是将 Query/Key 向量按其位置对应的角度进行高维空间的旋转，让向量的方向随位置变化，实现位置信息与语义信息的乘性融合（融合更紧密）

3. 点积自然体现相对位置：旋转后的 Query 和 Key 做注意力点积时，点积结果会自动包含两个 token 的相对位置信息（而非绝对位置），完美适配 Transformer 的注意力计算；

**最关键的设计**：RoPE 让注意力的点积满足**相对位置不变性**—— 对于任意两个 $tokeni$和 $j$，其注意力分数仅依赖于相对距离 $k=j−i$，与它们的绝对位置无关

### 传统位置编码的痛点

1. 正弦余弦位置编码

加性融合导致位置信息与语义信息解耦，长序列下位置表征效果衰减；仅能捕捉**绝对位置**，无法直接表达 token 间的相对位置。

2. 学习型位置编码

## 数学形式

### 二维空间的向量旋转

对于二维平面中的一个向量 $v = (x, y)$，将其绕原点逆时针旋转 $\theta$ 角后，得到新向量 $v'=(x', y')$，旋转公式由旋转矩阵给出：

$$v'=R(\theta)\cdot v=\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}
\cdot
\begin{pmatrix}
x \\
y
\end{pmatrix}
$$

### 为位置分配旋转角度

对于序列中位置为 $pos$的 token，我们为其设计旋转角度：

$$\theta_{pos}=pos\cdot w$$

其中 $w$为角速度，是一个固定值，用于控制旋转的速度。

为了和传统正弦余弦位置编码保持兼容性，RoPE 采用与之一致的角速度设计：

$$w_i=\frac{1}{10000^{2i/d_{model}}}$$

（二维场景下 $i=0$，即 $w=w_0​=1/10000^0=1$，高维场景会用到多个角速度）。

### 对 Query / key 做旋转（注入位置信息）

设 Transformer 中，位置为 $i$的 token 的 Query 向量为 $q_i$​，Key 向量为 $k_i$​（二维）；位置为 $j$ 的 token 的 Key 向量为 $k_j$​（二维）。

根据 RoPE 的思想，对 Query 和 Key 按其位置角度做旋转，旋转后的向量为：

$$q_{i}^{r}=R(\theta_i)\cdot q_i, k_{j}^{r}=R(\theta_j)\cdot k_j$$

其中 $\theta_i = i \cdot w, \theta_j = j \cdot w$，分别是位置 $i$ 和位置 $j$的旋转角度。

### 关键推导 —— 旋转后的点积包含相对位置

注意力的核心是 Query 与 Key 的点积，我们计算旋转后的 $q_{i}^{r}$ 和 k_{i}^{r} 的点积：

$$q_{i}^{r}\cdot k_{j}^{r}=[R(\theta_i)q_i]^T \cdot [R(\theta_j)k_j]$$

根据旋转矩阵的性质：旋转矩阵是正交矩阵，满足 $R(\theta)^T=R(-\theta)$，且 $R(\theta_1) \cdot R(\theta_2)=R(\theta_1 + theta_2)$，因此：

$$[R(\theta)]^T \cdot R(\theta_j)=R(-theta_i) \cdot R(\theta_j)=R(\theta_j - \theta_i)$$

带入点积公式得：

$$q_{i}^{r} \cdot k_{j}^r = q_{i}^T \cdot R(\theta_j - \theta_i)\cdot k_j$$

核心结论：

旋转后的 Query 和 Key 的点积，等价于原始 Query 与 “旋转了相对角度 $θ_{j−i}$ ​的原始 Key” 的点积，点积结果仅依赖于相对角度 $θ_{j−i}​=(j−i) \cdot w$，即仅依赖于两个 token 的相对距离 $k=j−i$，与它们的绝对位置 $i,j$无关！
这
意味着，RoPE 让自注意力天生捕捉相对位置信息，而这正是 Transformer 最需要的能力 —— 比如 “我吃苹果” 中，“吃”（i=2）和 “苹果”（j=3）的相对距离为 1，无论这两个 token 出现在序列的哪个位置，它们的注意力点积都只和相对距离 1 有关。

## 高维 RoPE 原理（核心，大模型实际使用）

实际的 Transformer 中，词嵌入的维度dmodel​是高维的（如 LLaMA 7B 的 $d_{model}​=4096$），而非二维。RoPE 的高维扩展思路非常巧妙：将高维向量按 2 个维度为一组进行配对，
每组独立执行二维 RoPE 旋转，不同组使用不同的角速度 $w_i$​，最终实现整个高维向量的位置编码。

这种分组旋转的方式，既保留了二维 RoPE 的相对位置特性，又实现了高维空间的位置编码，且计算量与维度线性相关，效率极高。

### 步骤 1：高维向量的分组配对

对于 $d_{model​}$ 维的向量 $v \in R^{d_{model}}$​（要求 $d_{model}$​为偶数，若为奇数可舍弃最后一维），将其按连续两个维度为一组，划分为 $d_{model}​/2$个二维子向量：

设隐藏维度为 $d$ (偶数)，位置为 $p$：

1. 定义角频率：

$$\theta_i = 10000^{-2i/d}$$

2. 对第 $i$ 对维度做旋转：

$$
\begin{pmatrix}
x_{2i}' \\
x_{2i+1}'
\end{pmatrix}
= \begin{pmatrix}
\cos(p\theta_i) & -\sin(p\theta_i) \\
\sin(p\theta_i) & \cos(p\theta_i)
\end{pmatrix}
\begin{pmatrix}
x_{2i} \\
x_{2i+1}
\end{pmatrix}
$$

这个旋转 只作用在 Q 和 K 上，不作用在 V 上。

### 为什么它是“相对位置编码”

关键性质：

$$\<R(p)q, R(k)k\>=<q, R(k-p)k>$$

### 和其他位置编码的对比

| 方法                         | 类型     | 特点         |
| -------------------------- | ------ | ---------- |
| 绝对位置编码（Sinusoidal）         | 绝对     | 简单，但外推能力有限 |
| Learned Position Embedding | 绝对     | 参数化，不能外推   |
| Relative Bias（T5）          | 相对     | 需额外 bias 表 |
| **RoPE**                   | **相对** | 无额外参数、外推性好 |

### 优点

+ ✅ 无额外可学习参数

+ ✅ 长上下文外推能力强

+ ✅ 与标准注意力机制完全兼容

+ ✅ 计算高效（只是旋转）

### 局限与改进

+ 原始 RoPE 在极长上下文下仍会退化

+ 因此出现了改进版本：

  + NTK-aware RoPE

  + XPos

  + YaRN

  + Dynamic RoPE scaling
