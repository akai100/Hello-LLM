
**残差连接** —— 它是 Transformer 能够构建深度模型（如 GPT-3 的 96 层、LLaMA 2 的 70 层）的核心技术之一，解决了深度神经网络的梯度消失 / 爆炸问题，同时保留底层特征信息。

## 1. 原理

残差连接最早由 ResNet 论文提出，Transformer 直接沿用并适配其架构特点，核心逻辑是：让网络层学习 “残差”（输入与输出的差值），而非直接学习完整的映射关系。

### 1.1 数学定义

### 1.2 残差连接位置

Transformer 的每一个子层（自注意力层、前馈网络层）都包裹残差连接 + 归一化，有两种经典范式：

+ Post-Norm

+ Pre-Norm

## 2. 作用

### 2.1 解决深度模型的[梯度消失](https://github.com/akai100/Hello-AI/blob/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%B6%E4%BB%96/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1.md)/爆炸问题


+ 梯度反向传播时，残差连接让梯度可直接通过 “短路路径”（ $x$ 的梯度 = 1）传回底层，无需经过复杂的映射函数 $F(x)$ 的导数链；

+ “1” 保证梯度不会因多层连乘趋近于 0 或无穷大；

### 2.2 保留底层特性，提升信息传递效率

+ Transformer 的自注意力层和 FFN 层会对输入做复杂变换（如注意力加权、非线性映射），残差连接确保 “原始输入信息” 不会被覆盖；

+ 例如：自注意力层专注学习上下文依赖的 “增量信息”，而残差连接保留输入的基础语义特征，两者结合后信息更完整；

### 2.3 加速模型收敛

+ 残差连接让模型在训练初期即可快速拟合简单映射（ $F(x)\aprox 0$ 时，$y≈x$），再逐步学习复杂的残差；

+ 对比无残差的模型，残差连接可使 Transformer 的收敛速度提升 30%-50%，尤其在大模型训练中（如 LLaMA 7B），能大幅减少训练步数；

### 2.4 增强模型的泛化能力

+ 残差连接本质是 “多路径融合”：底层特征可通过残差路径直接传递到高层，避免高层过度依赖当前层的变换，降低过拟合风险；

+ 实验验证：移除 Transformer 的残差连接后，模型在下游任务（如文本生成、翻译）的准确率会下降 10%-20%；
