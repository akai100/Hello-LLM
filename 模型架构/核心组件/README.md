## 相关面试题

### 1. 请比较一下几种常见的 LLM 架构，例如 Encoder-Only, Decoder-Only, 和 Encoder-Decoder，并说明它们各自最擅长的任务类型。

LLM的架构主要可以分为三类，它们的核心区别在于使用了Transformer的哪些部分以及注意力机制的类型，这直接决定了它们各自擅长的任务。

**1. Encoder-Only 架构 (例如 BERT, RoBERTa)**

+ 结构：由多个Transformer Encoder层堆叠而成
+ 核心机制：双向注意力机制。
  在处理序列中的任何一个词元时，模型都可以同时关注到它左边和右边的所有词元。这使得模型能够获得非常丰富的上下文表示。
+ 最擅长的任务类型：自然语言理解。
  + 具体任务：
    + 分类任务：情感分析、文本分类
    + 序列标注：命名实体识别
    + 句子关系判断：自然语言判断
    + 完形填空：像BERT的Masked Language Model (MLM) 预训练任务本身。
  + 原因：这些任务的核心是理解输入文本的深层含义，而双向上下文对于准确理解至关重要。这类模型的输出通常是固定的标签或类别，而非自由生成的长文本

  **2. Decoder-Only 架构（例如 GPT 系列，Llama，Qwen）**

  + **结构**： 由多个Transformer Decoder层堆叠而成，但移除了其中的Encoder-Decoder交叉注意力部分。
  + **核心机制： 单向（因果）自注意力机制 (Causal Self-Attention)**。在预测第 t 个词元时，模型只能关注到位置 1 到 t-1 的词元，不能看到未来的信息。这种自回归的特性天然适合生成任务。
  + **最擅长的任务类型：自然语言生成 (NLG)**
    + 具体任务：
      + 开放式文本生成： 写文章、故事、诗歌
      + 对话系统/聊天机器人： 如ChatGPT
      + 代码生成： 如Copilot
      + 上下文续写 (In-context Learning)
    + 原因：语言的生成过程是顺序的、从左到右的，Decoder-Only架构的单向注意力完美地模拟了这一过程。目前绝大多数的通用大语言模型都采用此架构

**3.  Encoder-Decoder 架构 (例如 T5, BART, 原始Transformer)**

+ 结构： 包含一个完整的Encoder栈和一个完整的Decoder栈
+ 核心机制： Encoder部分使用**双向注意力**来编码整个输入序列，形成一个全面的上下文表示。Decoder部分在生成输出时，
  一方面使用**单向注意力**处理已生成的序列，另一方面通过**交叉注意力** (Cross-Attention)机制来关注Encoder的输出，确保生成内容与输入相关。
+ 最擅长的任务类型：序列到序列 (Seq2Seq)。
  + 具体任务：
    + 机器翻译： 将一种语言（输入序列）翻译成另一种语言（输出序列）。
    + 文本摘要： 将一篇长文章（输入序列）概括成几句话（输出序列）。
    + 问答： 将问题（输入序列）转换为答案（输出序列）。
  + 原因： 这类任务需要首先对源序列有一个完整的、全局的理解（由Encoder完成），然后基于这个理解有条件地生成一个目标序列（由Decoder完成）。
  

  
