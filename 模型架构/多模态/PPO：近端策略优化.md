PPO（**Proximal Policy Optimization，近端策略优化**）是强化学习里**最常用、最稳健**的策略梯度算法之一，由 OpenAI 在 2017 年提出。它的目标很简单：**在保证训练稳定的前提下，高效地更新策略**。

## 1. PPO 要解决什么问题

在强化学习中，我们希望学习一个策略：

$$\pi(a|s)$$

来最大化期望回报

问题在于：

+ 梯度策略更新太猛：训练容易崩

+ 更新太保守：学得很慢

早期方法：

+ **REINFORCE**：方差巨大

+ **TRPO**：稳定但实现复杂（需要二阶优化、KL 约束）

**PPO 的定位**：

**像 TRPO 一样稳，但像普通 SGD 一样简单**

## 2. 核心思想

一句话总结：**新策略不能离旧策略太远**

如果新策略和旧策略差别太大：

+ 价值估计失效

+ 训练发散

PPO 用一个非常巧妙的方法来解决：

+ **不硬性约束策略变化**

+ **而是用一个裁剪（clip）的目标函数，惩罚过大的更新**

## 3. PPO 的数学形式

### 3.1 策略比率

定义：

$$r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

含义：

+ 衡量新策略相对于旧策略对某个动作的偏好变化

+ $r_t=1$：没变
+ $r_t > 1$：更偏好这个动作
+ $r_t < 1$：更不偏好

### 3.2 优势函数

$$A_t=Q(s_t,a_t)-V(s_t)$$

直觉：

+ $A_t > 0$：这个动作比平均好 -> 应该鼓励

+ $A_t < 0$：这个动作不咋地 → 应该抑制

通常用**GAE（Generalized Advantage Estimation）** 来估计。

### 3.3 PPO 的裁剪目标函数

$$L^{CLIP}(\theta)=E_t[min(r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]$$

**🧠 这一步到底在干嘛？**

+ 如果更新幅度在安全范围内 👉 正常更新

+ 如果更新太猛 👉 **直接截断收益**

📌 常见：

+ $\epsilon = 0.1 ~ 0.3$

### 3.4 完整 PPO 损失函数

实际训练时一般是三项之和：

$$L=L^{CLIP}-c_1L^{VF}+c_2H(\pi)$$

解释：

+ 策略损失（clip）

+ 价值函数损失（MSE）

+ 熵奖励（鼓励探索）

## 4. PPO 的算法流程

**🔁 每一轮训练（Iteration）：**

**1. 用当前策略采样轨迹**

+ 收集 $(s_t, a_t, r_t)$

**2. 计算回报 & 优势函数**

  + 通常用 GAE

**3. 固定旧策略**

   + 保存  $\pi \theta_{old}$

**4. 多次小批量更新**

   + 对同一批数据做多次 SGD

   + 使用裁剪目标函数

**5. 更新策略和价值网络**

📌 关键点：

+ **同一批数据可复用多次**

+ 比 TRPO 更 sample-efficient

## 5. PPO 的优点

**✅ 稳定**

+ 裁剪机制防止策略崩溃

**✅ 实现简单**

+ 一阶优化

+ 不需要 KL 的二阶近似

**✅ 泛用性强**

+ 连续 / 离散动作都行

+ 控制、游戏、NLP、RLHF 都能用

**✅ 工业级算法**

+ OpenAI、DeepMind、Meta 广泛使用

**👉 ChatGPT / RLHF 背后用的就是 PPO 变体**

## 6. PPO 的缺点

**❌ 仍然是 on-policy**

+ 数据不能长期复用

+ 比 SAC / DDPG 更吃样本

**❌ 超参数敏感**

+ clip 范围

+ GAE 参数

+ batch / mini-batch 大小

**❌ 探索能力有限**

+ 对稀疏奖励问题不如 off-policy 方法

## 7. PPO vs 其他算法

| 算法        | 稳定性 | 复杂度 | 样本效率 | 备注         |
| --------- | --- | --- | ---- | ---------- |
| REINFORCE | ❌   | ⭐   | ❌    | 理论简单       |
| TRPO      | ✅   | ❌❌  | ⭐    | 难实现        |
| **PPO**   | ✅   | ⭐⭐  | ⭐⭐   | **综合最优**   |
| DDPG      | ❌   | ⭐⭐  | ⭐⭐⭐  | 易不稳定       |
| SAC       | ✅   | ⭐⭐⭐ | ⭐⭐⭐⭐ | off-policy |

## 8. 什么时候用 PPO

**👉 优先用 PPO 的情况：**

+ 连续控制（机器人、仿真）

+ 策略网络较大

+ 想要稳定、好调参

+ RLHF / 对话 / 序列决策

**👉 不太适合 PPO：**

+ 极度稀疏奖励

+ 环境交互代价极高（考虑 off-policy）
