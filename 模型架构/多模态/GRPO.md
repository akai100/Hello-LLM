
## 1. GRPO 概述

GRPO（Group Relative Policy Optimization，组相对策略优化）是 DeepSeek 团队在 2024 年提出的一种强化算法，专门用于大语言模型（LLM）的偏好优化
与推理能力提升。它是**PPO（近端策略优化）的轻量级变体**，核心创新在于**完全去除独立的 critic（价值）网络**，通过组内相对奖励估计优势函数，
显著降低了训练资源消耗，同时提高了长序列推理任务的稳定性。

## 2. 核心原理与创新点

### 2.1 与 PPO 的核心区别


| 特性 | PPO | GRPO |
|------|----|-----|
| 价值估计	| 依赖独立训练的 critic 网络 | 无 critic 网络，使用组内平均奖励作为基线 |
| 优势计算	| 绝对价值与奖励的差值 | 组内归一化的相对奖励 |
| 资源消耗	| 高（需同时训练两个模型） | 低（仅训练 policy 模型） |
| 适用场景	| 通用强化学习任务 | 大模型推理、偏好对齐等长序列生成任务 |
| 训练稳定性	| 受 critic 网络性能影响 | 更稳定，避免价值估计偏差 |

### 2.2 核心机制

1. 组采样（Group Sampling）

对每个输入问题 q，从当前策略中采样**G 个候选输出**（一个 group），替代 PPO 的单路径采样。这样可以在同一问题下获得多样化的解决方案，为相对评估提供基础。

2. 相对优势估计（Relative Advantage Estimation）

GRPO 的核心创新点，计算公式为：

$$A_{i,j}=\frac{r_{i,j}-u_i}{\sigma_i + \epsilon}$$

其中：

+ $A_i,j$：第 $i$个问题第 $j$个输出的优势值
+ $r_i,j$：第 $i$个问题第 $j$个输出的奖励分数
+ $u_i$：第 $i$个问题所有 $G$个输出的平均奖励
+ $\sigma_i$：第 $i$个问题所有 $G$ 输出的奖励偏差
+ $\epsilon$：防止除以零的微小常数

这种组内归一化方式使模型学习**相对排名而非绝对分数**，契合奖励模型的比较性本质。

3. 策略更新机制

沿用 PPO 的裁剪目标函数，确保策略更新的稳定性：

$$J_{GRPO}(\theta)=E[min(r_t(\theta) \cdot A_t, clip(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \cdot A_t)=\beta \cdot KL(\pi_{\theta},\pi_{\theta_{old}})]$$

其中：

+ $r_t(\theta)=\frac{\pi_{\theta}(y_t|x,y<t)}{\pi_{\theta_{old}}(y_t|x,y<t)}$

+ clip：裁剪函数，限制策略更新幅度

+ $\beta$：KL 惩罚系数。防止模型生成不自然的文本

+ $\pi_{\theta_{old}}$：更新前的旧策略，用于稳定训练

### 3. 完整训练流程

GRPO 的训练遵循以下循环流程：

```
初始化策略模型π_θ0 → 循环N轮 → 结束
    1.  数据收集：对一批问题{q1, q2, ..., qM}，每个问题采样G个输出
    2.  奖励计算：使用奖励模型RM对每个输出评分{r11, r12, ..., rMG}
    3.  优势计算：按组归一化计算每个输出的优势值{A11, A12, ..., AMG}
    4.  策略更新：最大化GRPO目标函数，更新策略参数θ
    5.  策略固化：每隔K轮保存当前策略为旧策略π_θold
```
