## ViT æ˜¯ä»€ä¹ˆ

ViTï¼ˆVision Transformerï¼‰æŠŠå›¾åƒåˆ‡æˆ patchï¼Œå½“ä½œâ€œè¯åºåˆ—â€ï¼Œç”¨ Transformer æ¥åšè§†è§‰å»ºæ¨¡ã€‚

## ViT çš„æ•´ä½“æµç¨‹

ä¸€å¼ å›¾åƒ

â†’ Patch Embedding

â†’ åŠ ä½ç½®ç¼–ç 

â†’ Transformer Encoderï¼ˆå¤šå±‚ï¼‰

â†’ [CLS] token è¾“å‡ºåˆ†ç±»ç»“æœ


## æ ¸å¿ƒæ­¥éª¤æ‹†è§£

### 1ï¸âƒ£ Patch Embedding

è¾“å…¥å›¾åƒï¼š

$$x \in R^{H \times W \times C}$$

åˆ‡æˆå¤§å°ä¸º $P \times P$ çš„ patch:

$$N=\frac{HW}{P^2}$$

æ¯ä¸ª patch å±•å¹³ + çº¿æ€§æ˜ å°„ï¼š

$$x_p \in R^{P^2C} => D$$

ğŸ‘‰ å®ç°ä¸Šé€šå¸¸ç”¨ Conv2d(kernel=P, stride=P)

### 2ï¸âƒ£ åŠ  token & ä½ç½®ç¼–ç 

**[CLS] token**

+ ä¸€ä¸ªå¯å­¦ä¹ å‘é‡

+ ç”¨äºåˆ†ç±»ä»»åŠ¡

**Positional Embedding**

+ ViT ä½¿ç”¨å¯å­¦ä¹ ä½ç½®ç¼–ç 

+ å¿…é¡»æœ‰ï¼ˆTransformer ä¸æ„ŸçŸ¥é¡ºåºï¼‰

### 3ï¸âƒ£ Transformer Encoder

æ¯ä¸€å±‚åŒ…æ‹¬ï¼š

```
LayerNorm
Multi-Head Self Attention
Residual
MLP (FFN)
Residual
```

Attention å…¬å¼ï¼š

$$Attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d}})V$$

### 4ï¸âƒ£ è¾“å‡ºå±‚

+ åˆ†ç±»ï¼šå– [CLS] token

+ å›å½’ / denseï¼šç”¨æ‰€æœ‰ token

## ViT ä¸ºä»€ä¹ˆèƒ½ work

**1ï¸âƒ£ å…¨å±€å»ºæ¨¡èƒ½åŠ›å¼º**

+ CNNï¼šå±€éƒ¨æ„Ÿå—é‡

+ ViTï¼šä¸€å±‚å°±èƒ½çœ‹å…¨å›¾

**2ï¸âƒ£ Scaling å‹å¥½**

+ å‚æ•°é‡ â†‘ â†’ æ€§èƒ½ â†‘

+ å’Œ NLP Transformer ç±»ä¼¼

**3ï¸âƒ£ å¼± inductive bias**

+ ä¸å¼ºè¡Œå‡è®¾å¹³ç§»ä¸å˜æ€§

+ å¤§æ•°æ®ä¸‹æ›´çµæ´»

## ViT çš„ç¼ºç‚¹

| é—®é¢˜     | åŸå›                  |
| ------ | ------------------ |
| æ•°æ®ä¾èµ–å¤§  | inductive bias å°‘   |
| è®¡ç®—å¤æ‚åº¦é«˜ | Attention (O(N^2)) |
| å°æ•°æ®å·®   | ä¸å¦‚ CNN             |

## ViT vs CNN

| ç»´åº¦   | CNN | ViT           |
| ---- | --- | ------------- |
| å»ºæ¨¡   | å±€éƒ¨  | å…¨å±€            |
| Bias | å¼º   | å¼±             |
| æ•°æ®éœ€æ±‚ | å°‘   | å¤š             |
| å¯è§£é‡Šæ€§ | ä¸€èˆ¬  | Attention å¯è§†åŒ– |

## ç»å…¸æ”¹è¿› & å˜ç§

**ğŸ”¹ DeiT**

+ çŸ¥è¯†è’¸é¦

+ å°æ•°æ®è®­ç»ƒ ViT

**ğŸ”¹ Swin Transformer**

+ çª—å£ Attention

+ å±‚çº§ç»“æ„

**ğŸ”¹ PVT**

+ é‡‘å­—å¡”ç»“æ„

+ ä¸‹é‡‡æ · token

**ğŸ”¹ ViT-H / ViT-G**

+ å¤§æ¨¡å‹ Scaling
